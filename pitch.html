<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Pitch | Music and Science</title>
<meta name="author" content="Peter M. C. Harrison">
<meta name="description" content="In Section 7 we saw various forms of idealised waves: the sine wave, the sawtooth wave, and the square wave. These waves all differ in shape, but they share a key property: they are all periodic....">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Chapter 9 Pitch | Music and Science">
<meta property="og:type" content="book">
<meta property="og:description" content="In Section 7 we saw various forms of idealised waves: the sine wave, the sawtooth wave, and the square wave. These waves all differ in shape, but they share a key property: they are all periodic....">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Pitch | Music and Science">
<meta name="twitter:description" content="In Section 7 we saw various forms of idealised waves: the sine wave, the sawtooth wave, and the square wave. These waves all differ in shape, but they share a key property: they are all periodic....">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="libs/datatables-binding-0.31/datatables.js"></script><link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script><link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script><!-- It makes little sense to put CSS in an HTML file instead of a 
  CSS file, but this seems to be the only way I could get BS4 book to
  recognise it !--><style>
    .csl-entry {
      margin-bottom: 15px;
      padding-left: 30px;
      text-indent: -30px;
    }
  </style>
<script src="https://hypothes.is/embed.js" async></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Music and Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Overview</a></li>
<li><a class="" href="how-to-approach-the-courses.html"><span class="header-section-number">2</span> How to approach the courses</a></li>
<li><a class="" href="advice-on-essay-writing.html"><span class="header-section-number">3</span> Advice on essay writing</a></li>
<li><a class="" href="copyright.html"><span class="header-section-number">4</span> Copyright</a></li>
<li class="book-part">What is science?</li>
<li><a class="" href="science.html"><span class="header-section-number">5</span> Science</a></li>
<li><a class="" href="science-and-music.html"><span class="header-section-number">6</span> Science and music</a></li>
<li class="book-part">The science of music</li>
<li><a class="" href="foundations-of-acoustics.html"><span class="header-section-number">7</span> Foundations of acoustics</a></li>
<li><a class="" href="timbre.html"><span class="header-section-number">8</span> Timbre</a></li>
<li><a class="active" href="pitch.html"><span class="header-section-number">9</span> Pitch</a></li>
<li><a class="" href="consonance.html"><span class="header-section-number">10</span> Consonance</a></li>
<li><a class="" href="expectation.html"><span class="header-section-number">11</span> Expectation</a></li>
<li><a class="" href="evolution.html"><span class="header-section-number">12</span> Evolution</a></li>
<li><a class="" href="music-across-the-world.html"><span class="header-section-number">13</span> Music across the world</a></li>
<li><a class="" href="emotion.html"><span class="header-section-number">14</span> Emotion</a></li>
<li class="book-part">Scientific methods</li>
<li><a class="" href="introduction-1.html"><span class="header-section-number">15</span> Introduction</a></li>
<li><a class="" href="research-topics.html"><span class="header-section-number">16</span> Research topics</a></li>
<li><a class="" href="research-questions.html"><span class="header-section-number">17</span> Research questions</a></li>
<li><a class="" href="quantitativequalitative.html"><span class="header-section-number">18</span> Quantitative/qualitative</a></li>
<li><a class="" href="ethics.html"><span class="header-section-number">19</span> Ethics</a></li>
<li class="book-part">Quantitative methods</li>
<li><a class="" href="variables.html"><span class="header-section-number">20</span> Variables</a></li>
<li><a class="" href="causality.html"><span class="header-section-number">21</span> Causality</a></li>
<li><a class="" href="generalisability.html"><span class="header-section-number">22</span> Generalisability</a></li>
<li><a class="" href="data-visualisation.html"><span class="header-section-number">23</span> Data visualisation</a></li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">24</span> Descriptive statistics</a></li>
<li><a class="" href="inferential-statistics.html"><span class="header-section-number">25</span> Inferential statistics</a></li>
<li><a class="" href="appraising-limitations.html"><span class="header-section-number">26</span> Appraising limitations</a></li>
<li class="book-part">Computational approaches</li>
<li><a class="" href="computational-music-psychology.html"><span class="header-section-number">27</span> Computational music psychology</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/pmcharrison/intro-to-music-and-science">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="pitch" class="section level1">
<h1>
<span class="header-section-number">9</span> Pitch<a class="anchor" aria-label="anchor" href="#pitch"><i class="fas fa-link"></i></a>
</h1>
<p>In Section <a href="foundations-of-acoustics.html#foundations-of-acoustics">7</a> we saw various forms of idealised waves: the sine wave, the sawtooth wave, and the square wave. These waves all differ in shape, but they share a key property: they are all <em>periodic</em>. By periodic, we mean that the wave repeats itself at a regular time interval. This periodicity turns out to be integral to pitch perception.</p>
<div class="figure">
<img src="images/sine-wave-frequency=240.png" width="400" alt=""><p class="caption"><strong>An example sine wave.</strong> <audio controls controlslist="nodownload" style="display: block; margin-top: 10px"><source src="audio/tone-frequency=240.wav" type="audio/mpeg"></source></audio></p>
</div>
<p><br></p>

<p>We also covered the notion of ‘frequency’. Frequency tells us the repetition rate of a periodic waveform, and is expressed in units of Hz. A frequency of 5 Hz means that the waveform repeats at a rate of 5 times per second.</p>
<div class="figure">
<img src="images/sine-wave-frequency=5.png" width="400" alt=""><p class="caption"><strong>A sine wave with a frequency of 5 Hz.</strong></p>
</div>
<p><br></p>
<p>While frequency is a concrete, observable property in the outside world, pitch is something relatively intangible, present only in the mind. We define ‘pitch’ as the perceptual correlate of frequency; essentially, we are saying that “pitch is that perceptual quality of sound that derives from the underlying sound wave’s frequency”. </p>
<p>As discussed in <a href="foundations-of-acoustics.html#foundations-of-acoustics">7</a>, we can represent a given sound either in a <em>temporal</em> representation or in a <em>spectral</em> representation. In the temporal representation, we study the waveform, which tells us how pressure changes over time. In the spectral representation, we instead decompose the waveform into its constituent sine waves, and we study the frequencies and amplitudes of these sine waves. The process of converting from a temporal to a spectral representation is achieved using the mathematical technique of Fourier transformation.<br></p>
<div class="figure">
<img src="images/waveforms-to-spectra.png" style="width:100.0%" alt=""><p class="caption"><strong>Converting from temporal representations to spectral representations.</strong></p>
</div>
<p><br></p>
<p>The two main theories for human pitch perception correspond to these two different ways of representing sounds. We call these theories the <em>spectral</em> and <em>temporal</em> theories of pitch perception respectively.</p>
<div id="spectral-theory" class="section level2">
<h2>
<span class="header-section-number">9.1</span> Spectral theory<a class="anchor" aria-label="anchor" href="#spectral-theory"><i class="fas fa-link"></i></a>
</h2>
<p>According to the spectral theory, pitch perception depends fundamentally on a spectral analysis process that occurs within the <em>inner ear</em>, the deepest part of the ear. In mammals, the inner ear is encased in a bony structure called the <em>bony labyrinth</em>.</p>
<div class="figure">
<img src="images/inner-ear-location.png" style="width:100.0%" alt=""><p class="caption"><strong>The location of the inner ear (black rectangle).</strong> Credit: <a href="https://commons.wikimedia.org/wiki/File:Blausen_0329_EarAnatomy_InternalEar.png">Blausen.com staff</a>, <a href="https://creativecommons.org/licenses/by/3.0">CC BY 3.0</a>.</p>
</div>
<p><br></p>
<p>The inner ear contains various structures involved in both sound perception and in orientation perception. The spectral analysis process occurs specifically in the <em>cochlea</em>, this coiled structure at the end of the inner ear.</p>
<div class="figure">
<img src="images/inner-ear-and-cochlea.png" style="width:100.0%" alt=""><p class="caption"><strong>The location of the cochlea.</strong> Credit: <a href="https://commons.wikimedia.org/wiki/File:Gray920.png">Henry Vandyke Carter</a>, public domain.</p>
</div>
<p><br></p>
<p>It is easier to understand the cochlea if we imagine uncoiling it, as in the following diagram. The most important part of this diagram is the <em>basilar membrane</em>, a long structure that spans the length of the cochlea. At the base, the basilar membrane is thick and stiff, but at its apex, it is thin and mobile. As a result, the basilar membrane has different resonant properties along its length.</p>
<div class="figure">
<img src="images/basilar-membrane.png" style="width:100.0%" alt=""><p class="caption"><strong>Schematic illustration of the basilar membrane.</strong> Credit: <a href="https://commons.wikimedia.org/wiki/File:Schematic_uncoiled_cochlea.svg">Kern A, Heid C, Steeb W-H, Stoop N, Stoop R, derivative work by Mike.lifeguard</a>, <a href="https://creativecommons.org/licenses/by/2.5">CC BY 2.5</a>.</p>
</div>
<p><br></p>
<p>Specifically, the base of the basilar membrane resonates at high frequencies, but as we move from the base to the apex, the resonant frequencies become lower and lower. As a result, when sound enters the cochlea, its different spectral components are translated into resonances at different locations along the basilar membrane. This spatial localisation of frequency components is called <em>tonotopy</em>. These different locations have their own connections to nerve cells, which communicate the resonances towards the brain, with these resonances already separated by spectral frequency.</p>
<p><strong>Helmholtz’s <em>sympathetic resonance</em> theory</strong>. Helmholtz knew about the capacity of the ear to separate sounds by frequencies. He suggested that the ear contains many separate resonant elements, each tuned to a distinct characteristic frequency, similar to the strings in a piano. Incoming sound would then cause particular elements to resonate according to the sound’s spectral frequencies (Panel B in the figure below).</p>
<div class="figure">
<img src="images/resonance-and-travelling-waves.png" style="width:100.0%" alt=""><p class="caption"><strong>Schematic illustration of two theories of aural resonance.</strong> <strong>A:</strong> von Békésy’s travelling wave theory <span class="citation">(Békésy, <a href="#ref-Von_Bekesy1960-oh" role="doc-biblioref">1960</a>)</span>. <strong>B:</strong> Helmholtz’s <em>sympathetic resonance</em> theory <span class="citation">(Helmholtz, <a href="#ref-Helmholtz1875-fm" role="doc-biblioref">1875</a>)</span>. Credit: <span class="citation">Bell (<a href="#ref-Bell2004-wo" role="doc-biblioref">2004</a>)</span>, <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>.</p>
</div>
<p><br></p>
<p>Nowadays most people think that Helmholtz was partly right. We do think that the ear’s frequency dispersion capacity comes from the fact that different parts of the basilar membrane have different resonant frequencies. However, we no longer believe that the basilar membrane contains discrete resonators; rather, the basilar membrane’s physical properties gradually change as we progress from its base (thick, stiff) to its apex (thin, flexible), and this causes different locations to have different resonant frequencies.</p>
<p><strong>Travelling waves.</strong> The biophysicist von Békésy conducted a famous series of experiments from the 1940s to the 1960s where he dissected the inner ears of cadavers and used strobe photography to measure their vibrations in response to sounds. He verified that the basilar membrane indeed disperses different frequencies to different physical locations, and showed moreover that the membrane’s resonances correspond to a <em>travelling wave</em>, specifically a wave that travels down the length of the basilar membrane, analogous to what happens when you hold a rope at one end and flick it (Panel A in the figure above; see also <a href="https://isle.hanover.edu/Ch10AuditorySystem/Ch10TravellingWave.html">this link</a> for an interactive demo). This work ultimately won von Békésy the 1961 Nobel Prize in Physiology or Medicine. There is nonetheless still uncertainty about the precise physical mechanisms at play here, and debates about the extent to which Helmholtz’s and von Békésy’s theories characterise frequency dispersion in the cochlea; see <span class="citation">Bell (<a href="#ref-Bell2004-wo" role="doc-biblioref">2004</a>)</span> and <span class="citation">Babbs (<a href="#ref-Babbs2011-eg" role="doc-biblioref">2011</a>)</span> for some recent discussions.</p>
<p><strong>Auditory filters</strong>. A given location of the basilar membrane will, in practice, respond to a range of spectral frequencies centred on a particular <em>characteristic frequency</em>; put another way, it behaves like an <em>auditory filter</em>, filtering the frequency spectrum to include only a small range of frequencies. This frequency range is termed the <em>critical band</em>, and its width is termed the <em>critical bandwidth</em>, or sometimes the <em>equivalent rectangular bandwidth</em> (ERB). Critical bandwidth is closely linked to the perceptual sensitivity of the basilar membrane; when two tones fall within the same critical band, they both excite the same auditory filter, causing interference effects including masking (where one tone makes another tone sound quieter) and beating (i.e. fluctuations in sound amplitude). Critical bandwidth varies according to several factors; for example, louder sounds elicit a larger critical bandwidth, presumably because the increased energy spreads over a wide region of the basilar membrane. If we express critical bandwidths in musical semitones, we find average values of 2-4 semitones for ordinary musical registers, and we find that the number of semitones in the critical band <em>increases</em> for lower pitches, meaning that a given musical interval sounds ‘muddy’ and unclear if we play it low on the piano (or another instrument).</p>
<div class="figure">
<img src="images/band-pass-filter.svg" style="width:90.0%" alt=""><p class="caption"><strong>An idealised band-pass filter, used to model an auditory filter.</strong> <span class="math inline">\(F_C\)</span> is the characteristic frequency, and <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> correspond to the extrema of the critical band. Credit: <a href="https://en.wikipedia.org/wiki/File:Band-pass_filter.svg">Public domain.</a></p>
</div>
<p><br></p>
<p><strong>Inner hair cells.</strong> We have established that different parts of the basilar membrane oscillate in response to different spectral components. How does the ear measure those oscillations? If we look closely under a high-powered microscope, we find bundles of hairs sticking out of the basilar membrane. These tiny hairs are called <em>stereocilia</em>. They come in bundles (termed <em>hair bundles</em>), and are attached to <em>hair cells</em>. They look something like this:</p>
<div class="figure">
<img src="images/stereocilia-of-frog-inner-ear.jpg" style="width:100.0%" alt=""><p class="caption"><strong>High-resolution micrograph of stereocilia in a frog’s inner ear.</strong> Credit: Bechara Kachar, <a href="https://commons.wikimedia.org/wiki/File:Stereocilia_of_frog_inner_ear.01.jpg">public domain.</a></p>
</div>
<p><br></p>
<div class="figure">
<img src="images/hair-cells-wellcome-collection.jpg" style="width:100.0%" alt=""><p class="caption"><strong>Scanning electron microscope images of stereocilia.</strong> Top left: an outer hair cell from a guinea pig cochlea. Bottom left: an inner hair cell from a guinea pig cochlea. Right: a terrapin hair cell. Credit: David Furness, <a href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0.</a></p>
</div>
<p><br></p>
<p>Humans have two main types of hair cells: <em>inner hair cells</em> and <em>outer hair cells</em>. The inner hair cells are responsible for sensing oscillations in the basilar membrane.</p>
<p>The stereocilia of the inner hair cells project into the fluid-filled centre of the cochlea, termed the <em>cochlear duct</em>. When the inner hair cell vibrates, the stereocilia try to move along with it, but they drag against the cochlear fluid (<em>endolymph</em>).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Recent research suggests that the stereocilia may also be connected by filamentous structures to the overlaying &lt;em&gt;tectorial membrane&lt;/em&gt;, which would also exert similar resistance to movement &lt;span class="citation"&gt;(Hakizimana &amp;amp; Fridberger, &lt;a href="#ref-Hakizimana2021-wt" role="doc-biblioref"&gt;2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>4</sup></a> This causes the stereocilia to <em>shear</em>, i.e. to move away from their resting perpendicular position.</p>
<p>The tips of the stereocilia are connected by tiny filaments called <em>tip links</em>. When the stereocilia shear, this applies tension to the tip links, and causes <em>ion channels</em> to open, allowing positively charged <em>ions</em> (especially potassium, <span class="math inline">\(K^+\)</span>) to flow into the cell. This triggers the release of <em>neurotransmitters</em>, which in term cause the <em>auditory nerve</em> to fire.</p>
<div class="figure">
<img src="images/hair-cell-transduction.svg" style="width:60.0%" alt=""><p class="caption"><strong>Mechanotransduction in an inner hair cell.</strong> Credit: Thomas Haslwanter, <a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en">CC BY-SA 3.0.</a></p>
</div>
<p><br></p>
<p><strong>The auditory nerve.</strong> The auditory nerve is responsible for communication between the ear and the brain. It comprises a bundle of approximately 30,000 nerve fibres, most of which innervate inner hair cells, and the rest of which innervate outer hair cells. Different nerve fibres connect to different inner hair cells, each with their own characteristic frequencies; the auditory nerve therefore inherits the tonotopy of the basilar membrane, with different nerve fibres responding to different characteristic frequencies.</p>
<p><strong>Outer hair cells.</strong> The travelling wave mechanism identified by von Békésy appeared, on the face of it, to have two important limitations. The first is that a given spectral component would excite such a large portion of the basilar membrane that position information would only provide a very imprecise cue for spectral frequency. The second is that loud sounds would produce such immensely larger vibrations than quiet sounds that the latter would be very hard to detect. Recent decades of research indicate that the outer hair cells (found only in mammals) play a special role in counteracting these effects: they sharpen the physical response of the basilar membrane, meaning that spectral components excite a much smaller area, and they amplify the response to quiet sounds. The physical mechanisms of this sharpening and amplification are still being studied, but it is thought that they depend on the outer hair cells’ <em>electromobility</em>, namely their ability to change length in response to electrical stimulation. See <span class="citation">Dallos (<a href="#ref-Dallos2008-lk" role="doc-biblioref">2008</a>)</span> for a recent discussion.</p>
<p><strong>Deriving pitch from auditory nerve firings.</strong> As discussed previously, a complex tone has many spectral components. According to the above process, these spectral components excite different regions of the basilar membrane, stimulating different inner hair cells, and causing different nerve fibres in the auditory nerve to fire. The brain can thereby access its own spectral representation of the incoming sound: the spectral components simply correspond to the different excited nerve fibres. But how would the brain merge these all together to form a coherent percept of pitch? According to the <em>spectral</em> theory of pitch perception, the brain uses a <em>template-matching process</em>, where the template corresponds to a set of equally spaced harmonics, each corresponding to integer multiples of a common fundamental frequency, with amplitude decreasing as harmonic number increases. This template could either be innate or learned through experience.</p>
<div class="figure">
<img src="images/harmonic-spectrum-2.svg" width="450" alt=""><p class="caption"><strong>Notional harmonic template.</strong></p>
</div>
<p><br></p>
<p>The brain would continually search the spectrum for patterns that matched this template; whenever it identifies such patterns, it ‘merges’ their harmonics into a single auditory percept, corresponding to the complex tone. This percept would have a pitch corresponding to the lowest harmonic in this template, corresponding to the fundamental frequency.</p>
<p>This spectral theory of pitch perception is sometimes referred to as ‘place theory’ or ‘tonotopic theory’, to emphasise the way in which different spectral components are localised to different places in the ear.</p>
</div>
<div id="temporal-theory" class="section level2">
<h2>
<span class="header-section-number">9.2</span> Temporal theory<a class="anchor" aria-label="anchor" href="#temporal-theory"><i class="fas fa-link"></i></a>
</h2>
<p>Like the spectral theory, the temporal theory of pitch perception still relies on the mechanisms of the inner ear for translating air vibrations into neural impulses. However, it posits that pitch perception does not depend so much on transmitting the locations where the basilar membrane resonates, but rather depends on the temporal structure of the nerve impulses that are elicited by this resonance. In particular, the firing patterns of the nerve cells are known to entrain to the periodic motion of the basilar membrane, in what is called <em>phase locking</em>.</p>
<div class="figure">
<img src="images/phase-locking-intro.png" width="450" alt=""><p class="caption"><strong>Phase locking to an acoustic signal.</strong> Credit: <a href="https://commons.wikimedia.org/wiki/File:Phase_locking_recorded_from_a_neuron_in_the_cochlear_nucleus.jpg">Lvarnet</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>.</p>
</div>
<p><br></p>
<p>Though an individual nerve cell may not be able to fire fast enough to entrain to every wave cycle, multiple nerve cells stimulated together can simulate this effect, with different cells firing on different cycles. This idea that multiple nerve cells can work together to entrain to high frequencies is called <em>volley theory</em>. Through this mechanism, the brain can therefore access the main periodicities of the sound wave, represented as temporal firing patterns in the auditory nerve.</p>
<div class="figure">
<img src="images/volley-theory.png" style="width:100.0%" alt=""><p class="caption"><strong>Illustration of volley theory.</strong> Credit: <a href="https://commons.wikimedia.org/wiki/File:Volley_Principle_of_Hearing.png">Rachel Candace Law</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>.</p>
</div>
<p><br></p>
<p>Suppose that this temporal information does reach the brain – how does the brain then use this for pitch perception? The proposal is that the brain implements a version of <em>autocorrelation analysis</em>, which looks for time lags at which the signal correlates highly with itself. Time lags achieving high correlations are good candidates for the wave’s fundamental frequency.</p>
<div class="figure">
<img src="images/autocorrelation-analysis.png" width="500" alt=""><p class="caption"><strong>Illustration of autocorrelation analysis.</strong> The brackets point to time lags at which the signal correlates highly with itself.</p>
</div>
<p><br></p>
<p>There is a deep mathematical similarity between the autocorrelation analysis proposed here for the temporal theory and the Fourier analysis that was previously proposed for the spectral theory. We won’t go into the mathematical details here. However, one thing to point out is that the autocorrelation method doesn’t in general require the additional template-matching step that the Fourier method required.</p>
<p>Researchers have debated about spectral versus temporal mechanisms of pitch perception for almost a century, but remarkably we still don’t really know the extent to which either mechanism contributes to pitch perception. Even with modern neuroscientific methods, it is very difficult to unpick the brain’s implementation of pitch perception at a neural level, and so our understanding of this area relies in large part on combining results from many different behavioural psychoacoustics studies probing pitch perception in many different conditions. Currently it seems moderately plausible that both mechanisms contribute to pitch perception; in particular, it seems reasonable that the brain uses both mechanisms for frequencies up to about 2-4 kHz, at which point phase locking is thought to break down due to the neurochemical limitations of the interface between the basilar membrane and the auditory nerve <span class="citation">(Palmer &amp; Russell, <a href="#ref-Palmer1986-sd" role="doc-biblioref">1986</a>)</span>. At higher frequencies the place-based spectral mechanism would then take over.</p>
<p>Interestingly, 2-4 kHz is about the limit for pitch production in conventional musical instruments; for example, the highest note produced by the piccolo is about 4 kHz. One might therefore speculate that <em>musical</em> pitch relies particularly on temporal mechanisms.</p>
<div class="figure">
<img src="images/piccolo.jpg" style="width:100.0%" alt=""><p class="caption"><strong>Photo of a piccolo.</strong> Credit: <a href="https://commons.wikimedia.org/wiki/File:Piccolo_MET_254852.jpg">Metropolitan Museum of Art</a>, <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a>.</p>
</div>
<p><br></p>
</div>
<div id="pitch-intervals" class="section level2">
<h2>
<span class="header-section-number">9.3</span> Pitch intervals<a class="anchor" aria-label="anchor" href="#pitch-intervals"><i class="fas fa-link"></i></a>
</h2>
<p>Western listeners tend not to hear pitch in <em>absolute</em> terms; even highly trained musicians will struggle to name a given note played without additional contextual information. Instead, most Western listeners tend to hear pitch in <em>relative</em> terms, specifically in terms of the recognition of particular <em>frequency ratios</em>. For example, if a listener is played a pair of tones corresponding to the frequencies 555 Hz and 833 Hz respectively, they are unlikely to realise that the two tones correspond to a C# and a G#, but they might well recognise that the tones are separated by a perfect fifth.</p>
<p>Psychologically speaking, we describe pitch interval as the <em>perceptual correlates</em> of frequency ratio. For example, the octave corresponds to a 2:1 frequency ratio; if we take concert A (A4, 440 Hz), and double the frequency (880 Hz), this gives us the A one octave above (A5).</p>
<p>Music theorists have long been aware that, at least for Western listeners, certain pitch intervals seem to sound more pleasant (or harmonious) than others. This differentiation in pleasantness seems particularly strong when the two pitches are played simultaneously to produce <em>chords</em>. Intervals and chords that sound particularly pleasant are called <em>consonant</em>, and those that sound particularly unpleasant are called <em>dissonant</em>.</p>
<p>The consonance of an interval is deeply linked to its constituent frequency ratio. In particular, music theorists long ago established the following interesting observation: consonant intervals tend to have frequency ratios that can be approximated by a ratio of two small integers, for example 2:1, 3:2, 4:3, and so on. Such ratios are called <em>simple integer ratios.</em> We will discuss this phenomenon in more detail in Section <a href="consonance.html#consonance">10</a>.</p>
<p>Many intervals in Western music theory can be approximated by simple integer ratios such as these. Intervals with particularly simple integer ratios tend to be perceived as more consonant. The following table lists some examples with integer-ratio approximations as proposed by the Greek mathematician Ptolemy:</p>
<div class="inline-table"><table class="table table-sm">
<caption>Integer-ratio approximations for musical intervals, from Ptolemy’s intense diatonic scale.</caption>
<thead><tr class="header">
<th>Interval</th>
<th>Frequency ratio</th>
<th>Classification</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Unison</td>
<td>1:1</td>
<td>Consonant</td>
</tr>
<tr class="even">
<td>Major second</td>
<td>9:8</td>
<td>Dissonant</td>
</tr>
<tr class="odd">
<td>Major third</td>
<td>5:4</td>
<td>Consonant</td>
</tr>
<tr class="even">
<td>Perfect fourth</td>
<td>4:3</td>
<td>Consonant</td>
</tr>
<tr class="odd">
<td>Perfect fifth</td>
<td>3:2</td>
<td>Consonant</td>
</tr>
<tr class="even">
<td>Major sixth</td>
<td>5:3</td>
<td>Consonant</td>
</tr>
<tr class="odd">
<td>Major seventh</td>
<td>15:8</td>
<td>Dissonant</td>
</tr>
<tr class="even">
<td>Octave</td>
<td>2:1</td>
<td>Consonant</td>
</tr>
</tbody>
</table></div>
<p>This phenomenon of relative pitch has important implications for how we understand the concept of melody. On the one hand, any particular performance of a melody will be defined by a particular sequence of absolute pitches: for example, I might play the first phrase of ‘Old MacDonald’ starting on a concert A, giving me the following frequencies:</p>
<blockquote>
<p>440 Hz, 440 Hz, 440 Hz, 330 Hz, 367 Hz, 367 Hz, 330 Hz</p>
</blockquote>
<pre><code>#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command
#&gt; Warning in system(cmd): error in running command
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-1-be1f4c752f49.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-2-be1f60862da9.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-3-be1f720d7540.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-4-be1f2625cbb9.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-5-be1f67a885fb.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-6-be1f4df803f7.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-7-be1faccb420.wav', reason 'No such
#&gt; file or directory'</code></pre>
<audio controls controlslist="nodownload" style="display: block; margin-top: 10px"><source src="audio/old-macdonald-440.wav" type="audio/mpeg"></source></audio><p>However, I could equally well play the same melody starting on a different frequency, say 330 Hz:</p>
<blockquote>
<p>330 Hz, 330 Hz, 330 Hz, 247.5 Hz, 275.25 Hz, 275.25 Hz, 247.5 Hz</p>
</blockquote>
<pre><code>#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command

#&gt; Warning in system(.): error in running command
#&gt; Warning in system(cmd): error in running command
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-1-be1f6f557dab.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-2-be1f5a051fac.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-3-be1f776f7109.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-4-be1f4d65eb09.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-5-be1f5b9caed2.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-6-be1f6de6364.wav', reason 'No such
#&gt; file or directory'
#&gt; Warning in file.remove(files): cannot remove file
#&gt; '/tmp/Rtmp5BDjam/chord-7-be1f90ee40.wav', reason 'No such
#&gt; file or directory'</code></pre>
<audio controls controlslist="nodownload" style="display: block; margin-top: 10px"><source src="audio/old-macdonald-330.wav" type="audio/mpeg"></source></audio><p>Here we have changed all the frequencies of the melody by the same fixed ratio: 330/440. This changes the starting note but keeps all the intervals the same. We call this manipulation <em>transposition</em>.</p>
<p>For most Western listeners, melodies preserve their identities under transposition. So, it’s clear that the underlying cognitive representations of these melodies are not absolute frequencies. What else might they be?</p>
<p>One option is that listeners represent the melodies as intervals between successive notes. In that case, we could represent the melody as follows:</p>
<blockquote>
<p>Unison; Unison; Descending perfect fourth; Ascending major second; Unison; Descending major second</p>
</blockquote>
<p>Alternatively, listeners might represent the intervals as intervals from the tonic, which here would be the first note of the melody, giving us something like this:</p>
<blockquote>
<p>Unison; Unison; Perfect fourth below; Minor third below; Minor third below; Perfect fourth below</p>
</blockquote>
<p>It is not entirely clear yet what exact cognitive representation listeners use; the real answer seems to be complex and task-dependent <span class="citation">(see e.g. Dowling, <a href="#ref-dowling1978" role="doc-biblioref">1978</a><a href="#ref-dowling1978" role="doc-biblioref">a</a>)</span>. Nonetheless, it seems fair to say that some kind of intervallic representation must be involved, at least for Western listeners.</p>
</div>
<div id="pitch-notation" class="section level2">
<h2>
<span class="header-section-number">9.4</span> Pitch notation<a class="anchor" aria-label="anchor" href="#pitch-notation"><i class="fas fa-link"></i></a>
</h2>
<p>The pitch notation systems used in Western music reflect this primal role of pitch intervals in music perception. We split the continuous space of all possible musical pitches into a grid, where the basic unit in this grid is a fixed pitch interval, the <em>semitone</em>. The defining feature of the semitone is that it corresponds to a 12-fold division of the octave.</p>
<p>The piano keyboard provides a physical instantiation of this grid. Each successive key on the piano keyboard corresponds to one semitone above the previous key; moving from left to right takes us from low pitches to high pitches. Western music notation provides another instantiation of this grid, where instead low pitches are situated at the bottom of the stave and high pitches are situated at the top of the stave.</p>
<div class="figure">
<img src="images/piano-keyboard-with-notes.svg" style="width:100.0%" alt=""><p class="caption">Piano keyboard and musical stave. Credit: Artur Jan Fijałkowski, <a href="https://creativecommons.org/licenses/by-sa/2.5">CC BY-SA 2.5</a></p>
</div>
<p>It is useful to have standardised names for each of these notes. It is worth mentioning two main naming systems: <em>scientific pitch</em> <em>notation</em> and <em>MIDI pitch notation.</em> Scientific pitch notation assigns each pitch a letter, an optional accidental, and a number that designates the octave, following the scheme below:</p>
<div class="figure">
<img src="images/piano-octave-numbers.svg" alt=""><p class="caption">The octave numbering scheme used by scientific pitch notation. Credit: AlwaysAngry, <a href="https://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a></p>
</div>
<p>MIDI pitch notation assigns each note on the piano keyboard a single number, where successive integers correspond to successive semitones. For example, middle C would conventionally be 60, and concert A would be 69 (note that unfortunately there are some places have different conventions here, with middle C for example being given the number 48).</p>
<p>Historically speaking, different semitones would typically have slightly different sizes. The precise sizes of semitones would depend on the current tuning system. Nowadays, most of Western music uses twelve-tone equal temperament, where each semitone has exactly the same size. Unless otherwise specified, MIDI pitch notation is assumed to operate under this equal-temperament scheme too. This means we can have a simple mathematical formula that converts from MIDI pitch to frequency:</p>
<p><span class="math display">\[
\textrm{frequency} = 440 \times 2 ^ {(\textrm{MIDI} - 69) / 12}
\]</span></p>
<p>This formula works as follows: we are counting the number of semitones between our MIDI pitch and concert A, we’re dividing this by 12 to get the number of octaves, we’re expressing this as a power of 2 to get the frequency ratio, and we’re multiplying it by 440 (the frequency of concert A) to get the absolute frequency.</p>
<p>We can equivalently derive an inverse formula for computing the MIDI pitch for a given frequency:</p>
<p><span class="math display">\[
\textrm{MIDI pitch} = 
69 + 12 \log_2 \left( \frac{\textrm{frequency}}{440} \right)
\]</span></p>
<p>The following table illustrates these different notation systems for two octaves either side of middle C:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="center">MIDI note</th>
<th align="center">Scientific pitch notation</th>
<th align="center">Frequency (Hz)</th>
<th align="center">Also known as</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">48</td>
<td align="center">C3</td>
<td align="center">130.81</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">49</td>
<td align="center">C#3</td>
<td align="center">138.59</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">50</td>
<td align="center">D3</td>
<td align="center">146.83</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">51</td>
<td align="center">D#3</td>
<td align="center">155.56</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">52</td>
<td align="center">E3</td>
<td align="center">164.81</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">53</td>
<td align="center">F3</td>
<td align="center">174.61</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">54</td>
<td align="center">F#3</td>
<td align="center">185.00</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">55</td>
<td align="center">G3</td>
<td align="center">196.00</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">56</td>
<td align="center">G#3</td>
<td align="center">207.65</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">57</td>
<td align="center">A3</td>
<td align="center">220.00</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">58</td>
<td align="center">A#3</td>
<td align="center">233.08</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">59</td>
<td align="center">B3</td>
<td align="center">246.94</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">60</td>
<td align="center">C4</td>
<td align="center">261.63</td>
<td align="center">Middle C</td>
</tr>
<tr class="even">
<td align="center">61</td>
<td align="center">C#4</td>
<td align="center">277.18</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">62</td>
<td align="center">D4</td>
<td align="center">293.66</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">63</td>
<td align="center">D#4</td>
<td align="center">311.13</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">64</td>
<td align="center">E4</td>
<td align="center">329.63</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">65</td>
<td align="center">F4</td>
<td align="center">349.23</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">66</td>
<td align="center">F#4</td>
<td align="center">369.99</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">67</td>
<td align="center">G4</td>
<td align="center">392.00</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">68</td>
<td align="center">G#4</td>
<td align="center">415.30</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">69</td>
<td align="center">A4</td>
<td align="center">440.00</td>
<td align="center">Concert A</td>
</tr>
<tr class="odd">
<td align="center">70</td>
<td align="center">A#4</td>
<td align="center">466.16</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">71</td>
<td align="center">B4</td>
<td align="center">493.88</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">72</td>
<td align="center">C5</td>
<td align="center">523.25</td>
<td align="center"></td>
</tr>
</tbody>
</table></div>
<p>One useful thing about MIDI notation is that we can compute an intervallic representation of a melody simply by subtracting successive note numbers, which gives us the number of semitones between each note number. Another useful thing is that we can transpose a melody by simply adding or subtracting a fixed number to all notes.</p>
</div>
<div id="pitch-class-notation" class="section level2">
<h2>
<span class="header-section-number">9.5</span> Pitch-class notation<a class="anchor" aria-label="anchor" href="#pitch-class-notation"><i class="fas fa-link"></i></a>
</h2>
<p>Western music, and indeed many musical styles across the world, has an important notion of <em>octave equivalence.</em> Octave equivalence means that pitches separated by octaves (i.e. 2:1 frequency relationships) share some kind of underlying identity, which can be termed <em>chroma</em> <span class="citation">(Bachem, <a href="#ref-Bachem1950-eb" role="doc-biblioref">1950</a>)</span>. In Western music theory we use the term <em>pitch class</em> to denote collections of pitches separated by octaves. We can label pitch classes with their corresponding letter names (e.g. C, C#, D, D#, …), or alternatively we can label them with numbers, where by convention C is written as 0, C# is written as 1. A full table can be found below:</p>
<div class="inline-table"><table class="table table-sm">
<caption>Western pitch classes and their corresponding integer labels.</caption>
<thead><tr class="header">
<th>Letter name</th>
<th>Number</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>C</td>
<td>0</td>
</tr>
<tr class="even">
<td>C#</td>
<td>1</td>
</tr>
<tr class="odd">
<td>D</td>
<td>2</td>
</tr>
<tr class="even">
<td>D#</td>
<td>3</td>
</tr>
<tr class="odd">
<td>E</td>
<td>4</td>
</tr>
<tr class="even">
<td>F</td>
<td>5</td>
</tr>
<tr class="odd">
<td>F#</td>
<td>6</td>
</tr>
<tr class="even">
<td>G</td>
<td>7</td>
</tr>
<tr class="odd">
<td>G#</td>
<td>8</td>
</tr>
<tr class="even">
<td>A</td>
<td>9</td>
</tr>
<tr class="odd">
<td>A#</td>
<td>10</td>
</tr>
<tr class="even">
<td>B</td>
<td>11</td>
</tr>
</tbody>
</table></div>
<p>Mathematically, we can compute pitch class numbers from MIDI pitches by calculating the remainder when we divide the MIDI pitch by 12. For example, suppose we want to calculate the pitch class of MIDI pitch 62. We divide by 12; <span class="math inline">\(12 \times 5 = 60\)</span>, so we have 2 left over, meaning that our pitch class is 2. We express this using mathematical notation as follows:</p>
<p><span class="math display">\[
\textrm{pitch class} = \textrm{MIDI pitch} \mod 12
\]</span></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Babbs2011-eg">
<p>Babbs, C. F. (2011). Quantitative reappraisal of the helmholtz-guyton resonance theory of frequency tuning in the cochlea. <em>Journal of Biophysics</em>, <em>2011</em>, 435135. <a href="https://doi.org/10.1155/2011/435135">https://doi.org/10.1155/2011/435135</a></p>
</div>
<div id="ref-Bachem1950-eb">
<p>Bachem, A. (1950). Tone height and tone chroma as two different pitch qualities. <em>Acta Psychologica</em>, <em>7</em>, 80–88. <a href="https://doi.org/10.1016/0001-6918(50)90004-7">https://doi.org/10.1016/0001-6918(50)90004-7</a></p>
</div>
<div id="ref-Bell2004-wo">
<p>Bell, A. (2004). Hearing: Travelling wave or resonance? <em>PLoS Biology</em>, <em>2</em>(10), e337. <a href="https://doi.org/10.1371/journal.pbio.0020337">https://doi.org/10.1371/journal.pbio.0020337</a></p>
</div>
<div id="ref-Von_Bekesy1960-oh">
<p>Békésy, G. von. (1960). <em>Experiments in hearing</em>. McGraw-Hill.</p>
</div>
<div id="ref-Dallos2008-lk">
<p>Dallos, P. (2008). Cochlear amplification, outer hair cells and prestin. <em>Current Opinion in Neurobiology</em>, <em>18</em>(4), 370–376. <a href="https://doi.org/10.1016/j.conb.2008.08.016">https://doi.org/10.1016/j.conb.2008.08.016</a></p>
</div>
<div id="ref-dowling1978">
<p>Dowling, W. J. (1978a). Scale and contour: Two components of a theory of memory for melodies. <em>Psychological Review</em>, <em>85</em>(4), 341–354. <a href="https://doi.org/10.1037/0033-295x.85.4.341">https://doi.org/10.1037/0033-295x.85.4.341</a></p>
</div>
<div id="ref-Hakizimana2021-wt">
<p>Hakizimana, P., &amp; Fridberger, A. (2021). Inner hair cell stereocilia are embedded in the tectorial membrane. <em>Nature Communications</em>, <em>12</em>(1), 2604. <a href="https://doi.org/10.1038/s41467-021-22870-1">https://doi.org/10.1038/s41467-021-22870-1</a></p>
</div>
<div id="ref-Helmholtz1875-fm">
<p>Helmholtz, H. L. F. (1875). <em>On the sensations of tone as a physiological basis for the theory of music</em>. Longmans, Green; Co.</p>
</div>
<div id="ref-Palmer1986-sd">
<p>Palmer, A. R., &amp; Russell, I. J. (1986). Phase-locking in the cochlear nerve of the guinea-pig and its relation to the receptor potential of inner hair-cells. <em>Hearing Research</em>, <em>24</em>(1), 1–15. <a href="https://doi.org/10.1016/0378-5955(86)90002-x">https://doi.org/10.1016/0378-5955(86)90002-x</a></p>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="timbre.html"><span class="header-section-number">8</span> Timbre</a></div>
<div class="next"><a href="consonance.html"><span class="header-section-number">10</span> Consonance</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#pitch"><span class="header-section-number">9</span> Pitch</a></li>
<li><a class="nav-link" href="#spectral-theory"><span class="header-section-number">9.1</span> Spectral theory</a></li>
<li><a class="nav-link" href="#temporal-theory"><span class="header-section-number">9.2</span> Temporal theory</a></li>
<li><a class="nav-link" href="#pitch-intervals"><span class="header-section-number">9.3</span> Pitch intervals</a></li>
<li><a class="nav-link" href="#pitch-notation"><span class="header-section-number">9.4</span> Pitch notation</a></li>
<li><a class="nav-link" href="#pitch-class-notation"><span class="header-section-number">9.5</span> Pitch-class notation</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/pmcharrison/intro-to-music-and-science/blob/main/030-pitch.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/pmcharrison/intro-to-music-and-science/edit/main/030-pitch.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Music and Science</strong>" was written by Peter M. C. Harrison. It was last built on 2026-01-22.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
