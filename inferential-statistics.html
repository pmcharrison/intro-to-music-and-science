<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 25 Inferential statistics | Music and Science</title>
<meta name="author" content="Peter M. C. Harrison">
<meta name="description" content="25.1 What are inferential statistics? Many scientific questions can be formulated in terms of one or more populations which we want to understand. These could be populations of humans; for...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 25 Inferential statistics | Music and Science">
<meta property="og:type" content="book">
<meta property="og:description" content="25.1 What are inferential statistics? Many scientific questions can be formulated in terms of one or more populations which we want to understand. These could be populations of humans; for...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 25 Inferential statistics | Music and Science">
<meta name="twitter:description" content="25.1 What are inferential statistics? Many scientific questions can be formulated in terms of one or more populations which we want to understand. These could be populations of humans; for...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="libs/datatables-binding-0.20/datatables.js"></script><link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><script src="https://hypothes.is/embed.js" async></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Music and Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Overview</a></li>
<li><a class="" href="undergraduate-courses.html"><span class="header-section-number">2</span> Undergraduate courses</a></li>
<li><a class="" href="how-to-approach-the-courses.html"><span class="header-section-number">3</span> How to approach the courses</a></li>
<li><a class="" href="advice-on-essay-writing.html"><span class="header-section-number">4</span> Advice on essay writing</a></li>
<li><a class="" href="copyright.html"><span class="header-section-number">5</span> Copyright</a></li>
<li class="book-part">What is science?</li>
<li><a class="" href="science.html"><span class="header-section-number">6</span> Science</a></li>
<li><a class="" href="science-and-music.html"><span class="header-section-number">7</span> Science and music</a></li>
<li class="book-part">The science of music</li>
<li><a class="" href="foundations-of-acoustics.html"><span class="header-section-number">8</span> Foundations of acoustics</a></li>
<li><a class="" href="timbre.html"><span class="header-section-number">9</span> Timbre</a></li>
<li><a class="" href="pitch.html"><span class="header-section-number">10</span> Pitch</a></li>
<li><a class="" href="consonance.html"><span class="header-section-number">11</span> Consonance</a></li>
<li><a class="" href="evolution.html"><span class="header-section-number">12</span> Evolution</a></li>
<li><a class="" href="music-across-the-world.html"><span class="header-section-number">13</span> Music across the world</a></li>
<li><a class="" href="emotion.html"><span class="header-section-number">14</span> Emotion</a></li>
<li class="book-part">Scientific methods</li>
<li><a class="" href="introduction.html"><span class="header-section-number">15</span> Introduction</a></li>
<li><a class="" href="research-topics.html"><span class="header-section-number">16</span> Research topics</a></li>
<li><a class="" href="research-questions.html"><span class="header-section-number">17</span> Research questions</a></li>
<li><a class="" href="quantitativequalitative.html"><span class="header-section-number">18</span> Quantitative/qualitative</a></li>
<li><a class="" href="ethics.html"><span class="header-section-number">19</span> Ethics</a></li>
<li class="book-part">Quantitative methods</li>
<li><a class="" href="variables.html"><span class="header-section-number">20</span> Variables</a></li>
<li><a class="" href="causality.html"><span class="header-section-number">21</span> Causality</a></li>
<li><a class="" href="generalisability.html"><span class="header-section-number">22</span> Generalisability</a></li>
<li><a class="" href="data-visualisation.html"><span class="header-section-number">23</span> Data visualisation</a></li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">24</span> Descriptive statistics</a></li>
<li><a class="active" href="inferential-statistics.html"><span class="header-section-number">25</span> Inferential statistics</a></li>
<li class="book-part">Computational approaches</li>
<li><a class="" href="computational-music-psychology.html"><span class="header-section-number">26</span> Computational music psychology</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/pmcharrison/intro-to-music-and-science">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="inferential-statistics" class="section level1">
<h1>
<span class="header-section-number">25</span> Inferential statistics<a class="anchor" aria-label="anchor" href="#inferential-statistics"><i class="fas fa-link"></i></a>
</h1>
<div id="what-are-inferential-statistics" class="section level2">
<h2>
<span class="header-section-number">25.1</span> What are inferential statistics?<a class="anchor" aria-label="anchor" href="#what-are-inferential-statistics"><i class="fas fa-link"></i></a>
</h2>
<p>Many scientific questions can be formulated in terms of one or more <em>populations</em> which we want to understand. These could be populations of humans; for example, we might wish to understand the extent to which socioeconomic status predicts educational outcomes in the general population of schoolchildren. In the context of corpus analyses, these could be populations of music compositions; for example, we might study the extent to which parallel fifths occur in Baroque compositions versus Renaissance compositions.</p>
<p>A given scientific experiment can rarely work with an entire population at once. It is impractical for most scientists to collect educational outcomes for all schoolchildren in the world; likewise, it would be a nigh impossible task to compile digital encodings of all Baroque and Renaissance compositions in existence.</p>
<p>Instead, scientists normally work with <em>samples</em>. Samples are smaller datasets that are drawn from a particular population. The hope is that the sample is representative of the larger population, and that we can learn something from the sample that can be generalised to an insight about the population as a whole.</p>
<p><em>Inferential statistics</em> are a special family of statistics that are designed to help us with this generalisation task. They are intended to help us to understand when certain conclusions can be conclusively drawn from a given dataset, and when other conclusions cannot be drawn with any confidence.</p>
</div>
<div id="distributions" class="section level2">
<h2>
<span class="header-section-number">25.2</span> Distributions<a class="anchor" aria-label="anchor" href="#distributions"><i class="fas fa-link"></i></a>
</h2>
<p>When we talk about inferential statistics, we rely heavily on the mathematical concept of <em>probability distributions</em>. A probability distribution is a statistical model that tells us how our data samples are generated.</p>
<p>The most fundamental distribution in science is the <em>normal distribution</em>, also known as the <em>Gaussian distribution</em>. The normal distribution resembles a bell curve:</p>
<div class="inline-figure"><img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-2-1.png" width="100%"></div>
<p>The shape of this curve tells us what values are likely observations: in particular, the higher the probability density, the more likely the observation. When we generate samples from the probability distribution, they will cluster around these values.</p>
<div class="inline-figure"><img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-3-.gif" width="100%"></div>
<p>The normal distribution is defined by two parameters: its <em>mean</em> and its <em>standard deviation</em>. We encountered both of these concepts earlier in Chapter <a href="descriptive-statistics.html#descriptive-statistics">24</a>. The normal distributions above each have a mean of 0 and a standard deviation of 1.</p>
<p>The mean controls the location of the normal distribution. Here are some examples of normal distributions with different means:</p>
<div class="inline-figure"><img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-4-1.png" width="100%"></div>
<p>The standard deviation (often abbreviated to ’SD") controls the spread of the normal distribution. Here are some examples with different standard deviations:</p>
<div class="inline-figure"><img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-5-1.png" width="100%"></div>
<p>Much of inferential statistics can then be reduced to variants of the following logical process. We suppose that our population of interest can be modelled by some probability distribution, for example the normal distribution (which happens to resemble many real-world distributions strikingly well). We then try to <em>infer</em> the values of this distribution’s parameters, for example its mean and standard deviation, based on the data we observe.</p>
</div>
<div id="sample-size-and-uncertainty" class="section level2">
<h2>
<span class="header-section-number">25.3</span> Sample size and uncertainty<a class="anchor" aria-label="anchor" href="#sample-size-and-uncertainty"><i class="fas fa-link"></i></a>
</h2>
<p>The size of our sample is crucial for determining our ability to infer the value of a distribution’s parameters. When we have only a few data values, our observations will be dominated by random chance. When we have lots of data values, however, the power of averaging will overcome the noise in the individual samples.</p>
<p>For illustration, let’s suppose we are trying to use data to infer the mean of a normal distribution whose true value is 1.5. In the first case, we’ll plot the results from 5 experiments, each of which estimate the mean based on just 10 observations:</p>
<div class="inline-figure"><img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-6-1.png" width="100%"></div>
<p>These estimates of the mean are rather noisy, with a standard deviation of 0.240. We call this value the <em>standard error</em>; it tells us how unreliable our mean estimates are.</p>
<p>Now consider an analogous set of experiments, each with 1000 observations:</p>
<div class="inline-figure"><img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-7-1.png" width="100%"></div>
<p>Now the standard deviation of our means (our standard error) is 0.023, more-or-less a tenth of the original standard error. In other words, increasing our dataset size by a factor of 100 has made our estimates about 10 times more precise. There is a mathematical theorem called the <em>Central Limit Theorem</em> that formalises and generalises this observation, showing that in general if you multiply your sample size by <span class="math inline">\(N\)</span>, then your standard error will decrease by a factor of <span class="math inline">\(\sqrt{N}\)</span>.</p>
<p>The main takeaway message here is that increasing sample size reduces uncertainty. This is why sample size is considered a very important aspect of scientific studies; without sufficient sample sizes, we cannot rely on any of our conclusions.</p>
</div>
<div id="estimating-uncertainty" class="section level2">
<h2>
<span class="header-section-number">25.4</span> Estimating uncertainty<a class="anchor" aria-label="anchor" href="#estimating-uncertainty"><i class="fas fa-link"></i></a>
</h2>
<p>In the example above we estimated our uncertainty in our mean estimations by repeating the same experiment five times and computing the standard deviation of the means. This is not a very practical approach in real-life experiments, because if we had five identical datasets we’d probably want to compute one mean over all datasets (hence achieving a higher effective sample size) rather than computing means over 20% of the dataset at a time. In this case we wouldn’t be able to take our same approach of computing the standard deviation of the means, because we’d only have one mean.</p>
<p>Fortunately statisticians have developed various techniques that allow us to have our cake and eat it: to generate standard errors for our estimates while analysing the whole dataset at once. These techniques are mostly straightforward with standard statistical software.</p>
<p>In the simple case where we are computing the mean of a dataset, it turns out that the standard error of this mean can be estimated by taking the standard deviation and dividing it by the square root of the sample size. This is a useful corollary of the Central Limit Theorem mentioned above.</p>
<p>Here is an example of computing the standard error of the mean in R:</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">7</span>, <span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">5</span>, <span class="fl">6</span>, <span class="fl">2</span>, <span class="fl">3</span>, <span class="fl">4</span><span class="op">)</span>

<span class="va">sd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>
<span class="va">sd</span>
<span class="co">#&gt; [1] 1.581139</span>

<span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>
<span class="va">N</span> 
<span class="co">#&gt; [1] 9</span>

<span class="va">se</span> <span class="op">&lt;-</span> <span class="va">sd</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>
<span class="va">se</span>
<span class="co">#&gt; [1] 0.5270463</span></code></pre></div>
<p>In more complex data analysis methods, such as linear regression (see Section <a href="inferential-statistics.html#linear-regression">25.7</a>), most statistical software will return standard errors automatically, based again on various mathematical theorems.</p>
<p>A more general and powerful approach to computing standard errors is to use a technique called <em>bootstrapping</em>. Bootstrapping works by simulating thousands of artificial datasets and measuring the distribution of the relevant statistic in these simulations. We will not describe the details here, but it is worth being aware that the technique exists.</p>
</div>
<div id="representing-uncertainty" class="section level2">
<h2>
<span class="header-section-number">25.5</span> Representing uncertainty<a class="anchor" aria-label="anchor" href="#representing-uncertainty"><i class="fas fa-link"></i></a>
</h2>
<p>It is good practice to provide uncertainty estimates when reporting inferential statistics (e.g. means) in a scientific report. For example, we might report the mean of a dataset as follows:</p>
<blockquote>
<p>The mean reaction time among the musicians was 700 ms (<em>SD =</em> 633ms, <em>SE</em> = 76 ms).</p>
</blockquote>
<p>Here we have provided both the standard deviation (abbreviated as <em>SD</em>), which tells our reader about the spread of our data, and the standard error (abbreviated as <em>SE</em>), which tells our reader about the reliability of our mean estimate.</p>
<p>Often scientists will report something called a <em>confidence interval</em> instead of the standard error. A confidence interval provides a range of plausible values for a given statistic. Most commonly, scientists will report a 95% confidence interval; the idea behind a 95% confidence interval is that, if we repeated the same experiment infinitely many times, 95% of those repeats should give a mean within that confidence interval. As a rule of thumb, the confidence interval will typically correspond to the mean plus or minus 1.96 standard errors. So, if I have mean of 4.5 and a standard error of 1.5, then my 95% confidence interval will be [1.56, 7.44]. Confidence intervals can be computed automatically by most statistical software packages.</p>
<p>The general philosophy in scientific data analysis is to be conservative about interpreting the values of parameters. In particular, we try only to commit to statements that would only be true no matter what value we chose within the confidence interval. For example, suppose we had the following analysis:</p>
<blockquote>
<p>The musical intervention had a mean effect of 1.5 IQ points (95% confidence interval: [-0.2, 3.2]).</p>
</blockquote>
<p>In this case, we would generally <em>not</em> conclude that the musical intervention had a positive effect, even though the mean effect was indeed positive; this is because the 95% confidence interval still contains zero, so it is still plausible that the intervention had no effect.</p>
<p>It is conventional to represent uncertainty in plots using <em>error bars</em>. Here is an example of a plot containing error bars:</p>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-9-1.png" alt="Means for three conditions in a fictional dataset. The error bars denote 95% confidence intervals." width="100%"><p class="caption">
Figure 25.1: Means for three conditions in a fictional dataset. The error bars denote 95% confidence intervals.
</p>
</div>
<p>When including error bars in a plot, researchers have a choice of three kinds of uncertainty statistics: confidence intervals, standard errors, or standard deviations. When plotting a confidence interval, the two tails of the error bar correspond to the lower and higher bounds of the confidence interval respectively. When plotting a standard error, the lower tail corresponds to the mean minus one standard error, and the upper tail corresponds to the mean plus one standard error; an analogous approach is taken for plotting the standard deviation, as in the example above.</p>
<p>The choice of uncertainty statistic to include in a plot is primarily up to the researcher. It is essential, however, to specify clearly in the plot description which kind of statistic has been included.</p>
<!-- Moreover, scientists are often interested in a more abstract conception of populations, one which is not limited to the entities that exist at the present moment, but one that also includes entities that could plausibly exist. For example, a music theorist might conceptualise the population of Bach chorale harmonisations as including not only the chorale harmonisations that Bach actually wrote, but also the chorale harmonisations that he might have written had he had the chance.  -->
</div>
<div id="uncertainty-and-repeated-measures" class="section level2">
<h2>
<span class="header-section-number">25.6</span> Uncertainty and repeated measures<a class="anchor" aria-label="anchor" href="#uncertainty-and-repeated-measures"><i class="fas fa-link"></i></a>
</h2>
<p>A primary goal of experiment design is to reduce uncertainty. One way of reducing uncertainty is to take advantage of <em>repeated-measures</em> analyses. These analyses make intelligent use of repeated-measurement structure in the data to reduce uncertainty in parameters of interest.</p>
<p>Let’s consider a simple worked example. Suppose we have the following dataset, of IQ scores before and after a musical intervention:</p>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-10"></span>
<div id="htmlwidget-5e2ac1e8a8f8ea979905" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-5e2ac1e8a8f8ea979905">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100"],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],[84,103,105,115,102,94,102,121,137,112,115,105,153,122,78,125,111,80,98,110,107,119,109,113,103,109,129,120,124,119,134,113,110,127,84,96,111,105,120,117,108,105,118,112,120,108,109,118,131,133,118,103,129,133,112,121,104,121,110,106,107,82,107,110,90,114,133,117,125,99,118,92,114,81,111,127,100,103,106,123,118,110,105,132,116,111,130,118,103,115,142,109,104,119,98,128,100,92,113,122],[88,104,105,117,107,92,105,127,140,116,121,109,157,126,78,128,110,84,103,114,110,116,114,118,99,110,137,122,130,120,138,116,114,128,85,103,112,109,121,118,116,110,123,113,119,110,111,121,132,138,124,105,135,134,114,123,107,126,115,107,112,82,108,113,92,115,139,123,131,99,124,93,116,82,114,133,104,111,110,127,127,111,111,130,119,118,128,116,100,117,144,107,108,119,102,126,102,98,116,126]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Student<\/th>\n      <th>Pre-test IQ<\/th>\n      <th>Post-test IQ<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"searching":false,"rowId":false,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><p class="caption">
Figure 25.2: A fictional dataset of IQ scores before and after a musical intervention. The red rectangles denote 95% confidence intervals of the means.
</p>
</div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-10-2.png" alt="A fictional dataset of IQ scores before and after a musical intervention. The red rectangles denote 95% confidence intervals of the means." width="100%"><p class="caption">
Figure 25.2: A fictional dataset of IQ scores before and after a musical intervention. The red rectangles denote 95% confidence intervals of the means.
</p>
</div>
<p>Here the 95% confidence intervals of the means (plotted with red rectangles) overlap for the two datasets, so we can’t infer that there is any meaningful effect of the musical intervention on IQ scores.</p>
<p>However, suppose we construct a new analysis that better takes into account the repeated-measures structure of the data. In particular, suppose we compute a new variable, corresponding to the difference between pre-test and post-test IQ:</p>
<div id="htmlwidget-1e67eefdbd069228c986" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1e67eefdbd069228c986">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100"],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],[84,103,105,115,102,94,102,121,137,112,115,105,153,122,78,125,111,80,98,110,107,119,109,113,103,109,129,120,124,119,134,113,110,127,84,96,111,105,120,117,108,105,118,112,120,108,109,118,131,133,118,103,129,133,112,121,104,121,110,106,107,82,107,110,90,114,133,117,125,99,118,92,114,81,111,127,100,103,106,123,118,110,105,132,116,111,130,118,103,115,142,109,104,119,98,128,100,92,113,122],[88,104,105,117,107,92,105,127,140,116,121,109,157,126,78,128,110,84,103,114,110,116,114,118,99,110,137,122,130,120,138,116,114,128,85,103,112,109,121,118,116,110,123,113,119,110,111,121,132,138,124,105,135,134,114,123,107,126,115,107,112,82,108,113,92,115,139,123,131,99,124,93,116,82,114,133,104,111,110,127,127,111,111,130,119,118,128,116,100,117,144,107,108,119,102,126,102,98,116,126],[4,2,1,2,5,-2,3,6,3,4,6,5,4,4,-1,3,-0,4,5,4,3,-3,5,5,-4,1,7,2,6,1,4,2,4,2,1,7,1,4,1,1,7,5,5,1,-1,2,3,3,1,5,6,2,6,1,1,3,3,5,4,1,5,0,1,3,2,0,7,6,6,-1,6,1,2,1,3,6,4,8,4,4,8,1,6,-2,3,7,-2,-3,-2,2,2,-2,3,1,4,-3,3,6,3,4]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Student<\/th>\n      <th>Pre-test IQ<\/th>\n      <th>Post-test IQ<\/th>\n      <th>IQ change<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"searching":false,"rowId":false,"columnDefs":[{"className":"dt-right","targets":[1,2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><p>Then we can plot this new variable with a histogram:</p>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-12-1.png" alt="The same fictional dataset, but plotting changes in IQ scores. The red rectangle denotes the 95% confidence interval of the mean." width="100%"><p class="caption">
Figure 25.3: The same fictional dataset, but plotting changes in IQ scores. The red rectangle denotes the 95% confidence interval of the mean.
</p>
</div>
<p>Now we see that the 95% confidence interval for the change in IQs is [2.3, 3.4], which does not include zero. So, we can conclude that the musical intervention <em>did</em> indeed induce a statistically reliable increase in IQ scores.</p>
<p>Why did we gain so much precision in the latter analysis? In the original analysis, much of the noise in the data came from individual differences in student IQs, which dwarfed the small changes induced by the musical intervention. However, the latter analysis controlled for this variation by subtracting the pre-test IQ from the post-test IQ, producing a dataset that isolated <em>changes</em> in IQ. This gives us a much more reliable analysis of the effect of IQ.</p>
<p>Many repeated-measures analyses can be formulated using differencing analyses like the one above. More complex repeated-measures structures sometimes need more complex analysis methods, however. There is a general family of analyses called <em>mixed-effect</em> analyses which are particularly well-suited to such problems; we will not consider these methods here, but it’s useful to know that they exist.</p>
</div>
<div id="linear-regression" class="section level2">
<h2>
<span class="header-section-number">25.7</span> Linear regression<a class="anchor" aria-label="anchor" href="#linear-regression"><i class="fas fa-link"></i></a>
</h2>
<p>In science we often see variables that depend on multiple contributing factors, and we want to quantify the relevant <em>causal contributions</em> of each underlying factor. Linear regression is a technique designed to achieve exactly this.</p>
<p>Let’s consider the following example: we wish to understand how house prices depend both on floorspace and on number of rooms. Suppose we collect the following dataset of house prices:</p>
<div id="htmlwidget-58605ee4b53bff9828c9" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-58605ee4b53bff9828c9">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200"],[129,132,79,109,134,188,182,80,78,198,114,101,131,109,165,174,128,92,67,80,106,167,112,131,138,122,145,177,148,72,95,188,187,189,61,131,60,83,105,129,53,165,64,141,132,98,83,185,163,66,190,69,121,141,175,58,54,77,186,85,62,175,101,198,199,148,122,120,171,167,123,177,183,161,89,76,192,184,126,144,145,96,137,98,163,58,149,73,176,91,163,100,84,172,109,109,77,76,184,148,73,136,65,74,145,148,54,80,150,189,113,113,172,195,150,86,56,197,179,188,129,60,126,119,75,200,178,157,59,96,143,159,141,61,198,194,52,176,196,173,78,159,169,126,72,85,97,195,54,119,91,175,150,108,113,162,143,139,136,58,59,97,143,103,151,171,137,68,104,75,185,75,128,63,61,50,92,193,169,157,127,115,152,79,55,167,87,175,169,101,154,62,108,192,120,154,103,167,163,145],[3,4,2,4,5,5,6,3,2,6,3,3,4,3,6,5,4,3,1,3,2,5,3,4,3,4,4,6,5,2,2,7,6,6,2,5,1,2,2,5,2,5,2,5,4,2,2,6,6,2,6,2,3,4,6,1,1,2,7,3,1,6,3,7,6,5,4,4,6,6,4,6,5,5,3,1,6,5,4,5,5,2,5,3,5,2,4,2,6,2,6,4,2,5,3,2,2,2,6,4,2,3,1,2,4,5,2,2,4,7,3,3,6,6,4,2,1,6,6,6,4,2,4,4,2,7,5,4,1,2,6,6,4,1,7,7,1,5,7,6,2,5,5,3,1,2,2,8,1,4,4,5,4,3,2,5,4,3,4,1,1,2,5,2,5,6,4,2,3,3,5,2,4,2,1,1,2,5,6,5,4,4,4,2,1,5,2,6,6,3,5,1,3,6,3,4,4,5,6,4],["537,500","560,000","209,600","334,500","458,500","581,800","558,500","149,300","345,700","603,500","474,200","285,400","525,100","346,200","569,600","664,600","464,600","405,800","198,100","192,900","415,700","433,900","435,800","344,400","473,800","424,400","543,000","617,000","416,900","251,200","347,700","497,400","584,100","606,200","206,200","432,000","186,300","314,900","396,100","383,000","165,700","519,300","117,400","415,200","381,100","277,600","211,000","504,800","435,600","376,900","771,500","183,300","483,700","656,200","582,300","222,300","86,600","213,400","631,600","194,900","221,800","556,100","389,200","627,800","649,700","373,400","412,400","421,300","567,100","552,800","473,500","573,400","442,500","478,200","286,700","309,600","488,000","677,300","418,400","440,800","433,600","301,200","425,400","334,200","521,900","111,900","497,600","441,600","651,900","433,600","559,900","244,500","359,400","695,800","433,000","284,500","201,700","80,200","665,200","536,900","268,700","521,100","203,800","253,900","444,300","439,500","154,600","168,600","633,600","758,000","452,700","298,600","531,000","637,400","353,000","235,100","30,700","764,200","573,900","639,400","434,100","197,700","402,800","402,700","268,800","673,300","646,100","522,700","213,500","297,200","478,700","462,600","414,500","95,600","591,800","538,500","303,900","540,900","731,600","484,900","295,400","575,200","533,800","282,100","236,700","253,200","416,400","518,900","301,100","380,100","272,600","477,400","538,800","352,800","437,900","508,100","493,100","336,600","433,600","55,500","283,800","378,400","439,900","355,400","395,400","563,600","517,300","122,800","252,200","249,600","597,300","202,000","338,300","207,400","162,600","289,200","291,300","542,300","540,800","642,600","347,900","376,500","399,400","113,800","170,800","587,700","312,900","556,800","615,800","322,400","533,500","142,500","297,800","549,600","460,900","473,100","351,300","509,300","701,000","492,500"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>floorspace<\/th>\n      <th>rooms<\/th>\n      <th>price<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"searching":false,"rowId":false,"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><p>Let’s start out by computing pairwise correlations between the two predictor variables and price (the <em>outcome variable</em>):</p>
<p><img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-14-1.png" width="100%"><img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-14-2.png" width="100%"></p>
<p>In both cases, we see strong positive correlations: houses with more rooms tend to be priced higher, and houses with more floor space tend to be priced higher.</p>
<p>Let’s now look at the same data through the lens of linear regression. The primary purpose of linear regression is to estimate <em>marginal effects</em>. Marginal effects tell us how the outcome variable would change if we adjusted one predictor while holding all the others constant.</p>
<p>We begin by fitting the regression model:</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Fitting the regression model</span>
<span class="va">mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">price</span> <span class="op">~</span> <span class="va">rooms</span> <span class="op">+</span> <span class="va">floorspace</span>, data <span class="op">=</span> <span class="va">house_dataset</span><span class="op">)</span></code></pre></div>
<p>We then plot the marginal effect for floorspace. As before, we see a positive association: increasing floorspace causes the price to increase.</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Plotting marginal effects requires the ggeffects package,</span>
<span class="co"># you can install this with the following command:</span>
<span class="co">#</span>
<span class="co"># install.packages("ggeffects")</span>

<span class="op">(</span><span class="fu">ggeffects</span><span class="fu">::</span><span class="fu"><a href="https://strengejacke.github.io/ggeffects/reference/ggpredict.html">ggpredict</a></span><span class="op">(</span><span class="va">mod</span>, terms <span class="op">=</span> <span class="st">"floorspace"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">scale_x_continuous</span><span class="op">(</span><span class="st">"Floorspace"</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">scale_y_continuous</span><span class="op">(</span><span class="st">"Price"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">ggtitle</span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">theme_classic</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-16"></span>
<img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-16-1.png" alt="Marginal effect of floorspace on house price in a fictional dataset (95% confidence interval)." width="100%"><p class="caption">
Figure 25.4: Marginal effect of floorspace on house price in a fictional dataset (95% confidence interval).
</p>
</div>
<p>Now let’s plot the marginal effect of room number:</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="op">(</span><span class="fu">ggeffects</span><span class="fu">::</span><span class="fu"><a href="https://strengejacke.github.io/ggeffects/reference/ggpredict.html">ggpredict</a></span><span class="op">(</span><span class="va">mod</span>, terms <span class="op">=</span> <span class="st">"rooms"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">scale_x_continuous</span><span class="op">(</span><span class="st">"Number of rooms"</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">scale_y_continuous</span><span class="op">(</span><span class="st">"Price"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">ggtitle</span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">theme_classic</span><span class="op">(</span><span class="op">)</span> </code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-17"></span>
<img src="149-quantitative-vi-inferential_files/figure-html/unnamed-chunk-17-1.png" alt="Marginal effect of room number on house price in a fictional dataset (95% confidence interval)." width="100%"><p class="caption">
Figure 25.5: Marginal effect of room number on house price in a fictional dataset (95% confidence interval).
</p>
</div>
<p>Interestingly, the previous association between number of rooms and house price is <em>not</em> replicated. The overall trend is decreasing (more rooms yields a lower price), but there is a wide confidence interval and the marginal effect might well be zero.</p>
<p>What do we conclude from this? It seems that the real driver of house prices is floorspace, not the number of rooms. Houses with larger floorspace tend to contain more rooms, but having more rooms doesn’t make them more valuable <em>per se</em>. Linear regression is a valuable tool in situations such as this, where we have multiple variables relating in a complex fashion, and we wish to estimate their causal effects on each other.</p>
<div id="linear-regression-and-categorical-variables" class="section level3">
<h3>
<span class="header-section-number">25.7.1</span> Linear regression and categorical variables<a class="anchor" aria-label="anchor" href="#linear-regression-and-categorical-variables"><i class="fas fa-link"></i></a>
</h3>
<p>The example above illustrated linear regression with continuous predictor variables. However, it is also perfectly possible to perform linear regression with categorical predictor variables. In this case the marginal effect plot looks like a bar plot instead of a line plot.</p>
</div>
<div id="generating-predictions-from-regression-models" class="section level3">
<h3>
<span class="header-section-number">25.7.2</span> Generating predictions from regression models<a class="anchor" aria-label="anchor" href="#generating-predictions-from-regression-models"><i class="fas fa-link"></i></a>
</h3>
<p>We have discussed how regression models can be used for explaining relationships between variables. However, they can also be used for generating predictions for new data points. For example, I can use the regression model from above to predict the price of a house with a floorspace of 125 square metres and 5 rooms:</p>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod</span>, newdata <span class="op">=</span> <span class="fu">tibble</span><span class="op">(</span>rooms <span class="op">=</span> <span class="fl">5</span>, floorspace <span class="op">=</span> <span class="fl">125</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; [1] 391290</span></code></pre></div>
<p>The <code>equatiomatic</code> package in R provides a useful tool for extracting the equation used to generate these predictions:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># install.packages("equatiomatic")</span>

<span class="fu">equatiomatic</span><span class="fu">::</span><span class="fu"><a href="https://datalorax.github.io/equatiomatic/reference/extract_eq.html">extract_eq</a></span><span class="op">(</span><span class="va">mod</span>, use_coefs <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<p><span class="math display">\[
\operatorname{\widehat{price}} = -10396.62 - 13200.7(\operatorname{rooms}) + 3741.52(\operatorname{floorspace})
\]</span></p>
<p>If you wanted you could use this equation to generate your predictions manually.</p>
</div>
<div id="interactions" class="section level3">
<h3>
<span class="header-section-number">25.7.3</span> Interactions<a class="anchor" aria-label="anchor" href="#interactions"><i class="fas fa-link"></i></a>
</h3>
<p>An <em>interaction effect</em> is when the effect of one predictor depends on the value of another predictor. Interaction effects are easiest to conceptualise when one of the variables is continuous and one is categorical with <span class="math inline">\(N\)</span> levels. In this case, the interaction effect works by estimating <span class="math inline">\(N\)</span> versions of the continuous variable’s marginal effect, one for each of the <span class="math inline">\(N\)</span> levels of the categorical variable. However, interactions can in general occur between all kinds of predictor variables.</p>
</div>
<div id="nonlinear-regression" class="section level3">
<h3>
<span class="header-section-number">25.7.4</span> Nonlinear regression<a class="anchor" aria-label="anchor" href="#nonlinear-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Linear regression assumes that all the marginal effects are straight lines. However, it is possible to generalise regression to incorporate nonlinear (wiggly) marginal effects, using techniques such as <em>polynomial regression</em> and <em>generalised additive modelling</em>. We will not discuss these techniques here but they are conceptually similar to linear regression models aside from their ability to capture nonlinearities.</p>
</div>
<div id="choosing-appropriate-control-variables" class="section level3">
<h3>
<span class="header-section-number">25.7.5</span> Choosing appropriate control variables<a class="anchor" aria-label="anchor" href="#choosing-appropriate-control-variables"><i class="fas fa-link"></i></a>
</h3>
<p>Regression is often used to make causal inferences about particular domains. Many researchers are familiar with the idea that incorporating additional variables within a regression model, often termed ‘control variables’, can be an effective way to extract better causal inferences from observational data. However, it is less appreciated that incorporating inappropriate control variables can in fact <em>increase</em> inferential biases. It is worth doing some further reading about this topic if you are planning on such analyses in your own work, see for example <span class="citation">Wysocki et al. (<a href="references.html#ref-Wysocki2022-lu" role="doc-biblioref">2022</a>)</span>.</p>
</div>
</div>
<div id="null-hypothesis-significance-testing" class="section level2">
<h2>
<span class="header-section-number">25.8</span> Null hypothesis significance testing<a class="anchor" aria-label="anchor" href="#null-hypothesis-significance-testing"><i class="fas fa-link"></i></a>
</h2>
<p><em>Null hypothesis significance testing</em> is a particular approach to inferential statistics that dominated psychological research for much of the 20th century, and is still very prevalent today. Many modern statisticians however now have deep reservations about this approach, arguing that it does not answer the questions that most scientists actually want to answer, and that it encourages dangerous statistical misconceptions among the researchers that use it. These issues notwithstanding, the great majority of existing literature use these or related techniques, and so it is important to understand them even if one prefers not to use them oneself.</p>
<p>The basic principle in null hypothesis testing is that we use our data to falsify a <em>null hypothesis</em> that claims that our dataset is statistically homogeneous. For example, suppose that we are examining differences in means between two experimental conditions, ‘A’ and ‘B’. The null hypothesis is that these two conditions have the same population means. If our data falsify this hypothesis, then we can be confident that some difference exists between the means.</p>
<p>Null hypothesis significance testing relies particularly on the notion of the <em>p-value</em>. The p-value is a special statistic with a subtle meaning that many researchers misremember. It can be defined as follows:</p>
<blockquote>
<p>The p-value is the probability that an effect equally extreme or more extreme than the present effect would have been observed were the null hypothesis to be true.</p>
</blockquote>
<p>Suppose for example that we observed a difference of 3.2 IQ points between our conditions ‘A’ and ‘B’. The p-value would then tell us the probability that we’d observe an IQ difference of 3.2 or greater in the hypothetical world where ‘A’ and ‘B’ had the same population means.</p>
<p>Small p-values constitute stronger evidence against the null hypothesis. For example, <span class="math inline">\(p = .001\)</span> means that we’d only see such a strong effect 1 out of 1000 times if the null hypothesis were true. This is very unlikely, suggesting that we must seriously consider the alternative: that a population difference does exist between the conditions.</p>
<p>It is conventional to use fixed thresholds for interpreting p-values, in particular the threshold <span class="math inline">\(p = .05\)</span>. If the p-value is less than .05, it is conventional to say that the effect is <em>statistically significant</em>. In this case, we are ‘permitted’ to say that we observed an effect in the data:</p>
<blockquote>
<p>Members of the intervention group exhibited statistically significant improvements in IQ scores (<span class="math inline">\(p = .02\)</span>).</p>
</blockquote>
<p>In contrast, when the p-value is greater than .05, the effect does not reach statistical significance, and we are not allowed to claim that we saw an effect.</p>
<blockquote>
<p>Members of the intervention exhibited no statistically significant improvement in IQ scores over the control group (<span class="math inline">\(p = .25\)</span>).</p>
</blockquote>
<p>In certain kinds of analyses (for example so-called <em>post-hoc tests</em>) this thresholding process is nuanced by the use of so-called <em>familywise error correction</em>, intended to control the level of false positives in a given analysis. We need not worry about the details of this here.</p>
<p>Null hypothesis significance testing is associated with a range of different statistical methods in the literature designed to work with different kinds of experiment designs. Examples include:</p>
<ul>
<li>t-tests</li>
<li>ANOVAs</li>
<li>Wilcoxon tests</li>
<li>Friedman tests</li>
<li>Kruskal-Wallace tests</li>
</ul>
<p>Though the details of these methods vary, they all try to address a similar question: do the different experimental conditions have different population parameters, or do they have the same population parameters. You will generally see them reported with p-values, which you can interpret according to the definition provided above.</p>
<p>The use of null hypothesis significance testing in psychology is increasingly controversial. Arguably the biggest issue is that it works by trying to disprove a straw-man hypothesis. In psychology, almost any variable is correlated to some degree with almost any other variable, and almost any intervention will have <em>some kind</em> of effect if you look closely enough. It is not interesting to know whether the effect is absolutely zero or not, because we know that there will almost always be some effect. Rather, we want to know <em>how big</em> the effect is, and in what direction it lies. The p-value tells us nothing about this. This is why we strongly encourage the readers of the present text to focus on reporting confidence intervals in their analyses, and to design their studies around measuring the size of effects rather than testing whether effects exist in the first place.</p>

</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="descriptive-statistics.html"><span class="header-section-number">24</span> Descriptive statistics</a></div>
<div class="next"><a href="computational-music-psychology.html"><span class="header-section-number">26</span> Computational music psychology</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#inferential-statistics"><span class="header-section-number">25</span> Inferential statistics</a></li>
<li><a class="nav-link" href="#what-are-inferential-statistics"><span class="header-section-number">25.1</span> What are inferential statistics?</a></li>
<li><a class="nav-link" href="#distributions"><span class="header-section-number">25.2</span> Distributions</a></li>
<li><a class="nav-link" href="#sample-size-and-uncertainty"><span class="header-section-number">25.3</span> Sample size and uncertainty</a></li>
<li><a class="nav-link" href="#estimating-uncertainty"><span class="header-section-number">25.4</span> Estimating uncertainty</a></li>
<li><a class="nav-link" href="#representing-uncertainty"><span class="header-section-number">25.5</span> Representing uncertainty</a></li>
<li><a class="nav-link" href="#uncertainty-and-repeated-measures"><span class="header-section-number">25.6</span> Uncertainty and repeated measures</a></li>
<li>
<a class="nav-link" href="#linear-regression"><span class="header-section-number">25.7</span> Linear regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#linear-regression-and-categorical-variables"><span class="header-section-number">25.7.1</span> Linear regression and categorical variables</a></li>
<li><a class="nav-link" href="#generating-predictions-from-regression-models"><span class="header-section-number">25.7.2</span> Generating predictions from regression models</a></li>
<li><a class="nav-link" href="#interactions"><span class="header-section-number">25.7.3</span> Interactions</a></li>
<li><a class="nav-link" href="#nonlinear-regression"><span class="header-section-number">25.7.4</span> Nonlinear regression</a></li>
<li><a class="nav-link" href="#choosing-appropriate-control-variables"><span class="header-section-number">25.7.5</span> Choosing appropriate control variables</a></li>
</ul>
</li>
<li><a class="nav-link" href="#null-hypothesis-significance-testing"><span class="header-section-number">25.8</span> Null hypothesis significance testing</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/pmcharrison/intro-to-music-and-science/blob/main/149-quantitative-vi-inferential.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/pmcharrison/intro-to-music-and-science/edit/main/149-quantitative-vi-inferential.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Music and Science</strong>" was written by Peter M. C. Harrison. It was last built on 2022-11-02.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
