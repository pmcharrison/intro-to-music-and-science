<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 15 Emotion | Music and Science</title>
<meta name="author" content="Peter M. C. Harrison">
<meta name="description" content="15.1 Introduction to emotion and music We’re very used to describing emotions with language. We have many different words to use, each of which evokes a particular facet of human emotional...">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Chapter 15 Emotion | Music and Science">
<meta property="og:type" content="book">
<meta property="og:description" content="15.1 Introduction to emotion and music We’re very used to describing emotions with language. We have many different words to use, each of which evokes a particular facet of human emotional...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 15 Emotion | Music and Science">
<meta name="twitter:description" content="15.1 Introduction to emotion and music We’re very used to describing emotions with language. We have many different words to use, each of which evokes a particular facet of human emotional...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="libs/datatables-binding-0.31/datatables.js"></script><link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script><link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script><!-- It makes little sense to put CSS in an HTML file instead of a 
  CSS file, but this seems to be the only way I could get BS4 book to
  recognise it !--><style>
    .csl-entry {
      margin-bottom: 15px;
      padding-left: 30px;
      text-indent: -30px;
    }
  </style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Music and Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Overview</a></li>
<li><a class="" href="undergraduate-courses.html"><span class="header-section-number">2</span> Undergraduate courses</a></li>
<li><a class="" href="how-to-approach-the-courses.html"><span class="header-section-number">3</span> How to approach the courses</a></li>
<li><a class="" href="advice-on-essay-writing.html"><span class="header-section-number">4</span> Advice on essay writing</a></li>
<li><a class="" href="copyright.html"><span class="header-section-number">5</span> Copyright</a></li>
<li class="book-part">What is science?</li>
<li><a class="" href="science.html"><span class="header-section-number">6</span> Science</a></li>
<li><a class="" href="science-and-music.html"><span class="header-section-number">7</span> Science and music</a></li>
<li class="book-part">The science of music</li>
<li><a class="" href="foundations-of-acoustics.html"><span class="header-section-number">8</span> Foundations of acoustics</a></li>
<li><a class="" href="timbre.html"><span class="header-section-number">9</span> Timbre</a></li>
<li><a class="" href="pitch.html"><span class="header-section-number">10</span> Pitch</a></li>
<li><a class="" href="consonance.html"><span class="header-section-number">11</span> Consonance</a></li>
<li><a class="" href="expectation.html"><span class="header-section-number">12</span> Expectation</a></li>
<li><a class="" href="evolution.html"><span class="header-section-number">13</span> Evolution</a></li>
<li><a class="" href="music-across-the-world.html"><span class="header-section-number">14</span> Music across the world</a></li>
<li><a class="active" href="emotion.html"><span class="header-section-number">15</span> Emotion</a></li>
<li class="book-part">Scientific methods</li>
<li><a class="" href="introduction-1.html"><span class="header-section-number">16</span> Introduction</a></li>
<li><a class="" href="research-topics.html"><span class="header-section-number">17</span> Research topics</a></li>
<li><a class="" href="research-questions.html"><span class="header-section-number">18</span> Research questions</a></li>
<li><a class="" href="quantitativequalitative.html"><span class="header-section-number">19</span> Quantitative/qualitative</a></li>
<li><a class="" href="ethics.html"><span class="header-section-number">20</span> Ethics</a></li>
<li class="book-part">Quantitative methods</li>
<li><a class="" href="variables.html"><span class="header-section-number">21</span> Variables</a></li>
<li><a class="" href="causality.html"><span class="header-section-number">22</span> Causality</a></li>
<li><a class="" href="generalisability.html"><span class="header-section-number">23</span> Generalisability</a></li>
<li><a class="" href="data-visualisation.html"><span class="header-section-number">24</span> Data visualisation</a></li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">25</span> Descriptive statistics</a></li>
<li><a class="" href="inferential-statistics.html"><span class="header-section-number">26</span> Inferential statistics</a></li>
<li><a class="" href="appraising-limitations.html"><span class="header-section-number">27</span> Appraising limitations</a></li>
<li class="book-part">Computational approaches</li>
<li><a class="" href="computational-music-psychology.html"><span class="header-section-number">28</span> Computational music psychology</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/pmcharrison/intro-to-music-and-science">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="emotion" class="section level1">
<h1>
<span class="header-section-number">15</span> Emotion<a class="anchor" aria-label="anchor" href="#emotion"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-to-emotion-and-music" class="section level2">
<h2>
<span class="header-section-number">15.1</span> Introduction to emotion and music<a class="anchor" aria-label="anchor" href="#introduction-to-emotion-and-music"><i class="fas fa-link"></i></a>
</h2>
<p>We’re very used to describing emotions with language. We have many different words to use, each of which evokes a particular facet of human emotional experience. If I say the word ‘disgust’, you know exactly what I mean – that feeling when I bite into a rotten apple, or open the rubbish bin, or step in dog faeces. If I say the word ‘amazement’, you likewise know instantly what I’m talking about: a special combination of surprise and disbelief.</p>
<p>Music’s capacity to express emotion is very different. On the one hand, it lacks some of the expressive specificity of language. It’s quite hard to write a melody that unambiguously connotes the emotion of ‘disgust’, for example.</p>
<p>Music does have some special advantages in its relationship with emotion, though. One feature is a particularly strong power to induce certain emotions in the audience. This feature underpins ways in which music is used across the world, for example in the context of lullabies, love songs, war songs, and religious ceremonies. In Western society, we regularly experience this emotional power in the context of TV and film. As audience members, we are so used to our emotions being manipulated in this way that we barely notice it any more.</p>
<p>This power of emotion induction becomes particularly interesting when the music is created interactively by several people playing or singing together. The music becomes a vehicle both for expressing one’s own emotions, and for experiencing emotions expressed by others in the group. The result is a special kind of shared, synchronous emotional experience.</p>
<p>We’re going to focus on a few key topics in this chapter. We’ll start by covering definitions of core concepts, such as emotion, mood, and affect. We’ll go through several current psychological models of emotion, some of which are general psychological models, and some of which are specific to aesthetics or to music. We’ll discuss various methods for studying emotions in laboratory contexts, many of which are well-suited to musical applications. Finally, we’ll discuss specific mechanisms by which music induces emotion.</p>
<div id="definitions" class="section level3">
<h3>
<span class="header-section-number">15.1.1</span> Definitions<a class="anchor" aria-label="anchor" href="#definitions"><i class="fas fa-link"></i></a>
</h3>
<p>Many scientific terms related to emotion do have intuitive meanings from day-to-day life. We do have to be careful though when we apply these terms in a scientific context – we need to be quite precise about what we mean, otherwise we can end up tying ourselves in knots. This is all made particularly difficult by the fact that there’s still not a complete scientific consensus about what many of these terms mean.</p>
<div id="emotion-1" class="section level4">
<h4>
<span class="header-section-number">15.1.1.1</span> Emotion<a class="anchor" aria-label="anchor" href="#emotion-1"><i class="fas fa-link"></i></a>
</h4>
<p>The <a href="https://dictionary.apa.org">APA Dictionary of Psychology</a> defines emotion as “a complex reaction pattern, involving experiential, behavioral, and physiological elements, by which an individual attempts to deal with a personally significant matter or event.”</p>
<p>What do we mean by a reaction pattern? Let’s take ‘fear’ as an example. As stated in the definition, the reaction pattern must comprise three components: experiential, behavioural, and physiological.</p>
<ul>
<li><p>The <em>experiential</em> component is what the mind has conscious access to. When I say “I feel scared”, I’m giving a label to the subjective experience of fear. Typically the emotion will be grounded in a particular object, event, or circumstance. For example, I might experience fear of a snake, or fear of stepping onstage to give a musical performance.</p></li>
<li><p>The <em>behavioural</em> component concerns what physical actions we perform with our body. Experiencing an emotion doesn’t mean that we have to enact these physical actions; instead, the emotion provides a ‘tendency’ to enact these actions. In the case of fear, one action tendency might be to freeze, so that I don’t make the situation any worse by disturbing the snake. A second action tendency might be to make a particular facial expression, to communicate my fear to other members of my species (or ‘conspecifics’, as we can call them). If I’m particularly scared, I might also make a vocal expression, like a scream. This has a similar function of communicating to conspecifics.</p></li>
<li><p>The <em>physiological</em> component prepares the body in various ways for future action. In the case of fear, this could mean a raised heart rate, increased perspiration, suppressed digestion, and various other kinds of things that should help me to respond to the salient threat.</p></li>
</ul>
<p>We can identify several important functions of these reaction patterns in survival contexts:</p>
<ul>
<li><p>One is <strong>preparing the body</strong> for an anticipated future. For example, a raised heart rate in the context of fear prepares the individual for subsequent physical exertion, so that they can overcome the current threat.</p></li>
<li><p>A second important function is <strong>signalling to others</strong>. This will typically be through some combination of facial and verbal expressions. Signalling emotions is very useful in a social context; it helps individuals to work together to overcome threats, and it helps them to build stable social relationships built on mutual trust.</p></li>
<li><p>A third function is <strong>promoting desirable behaviour</strong> in the individual. For example, a fear of heights is adaptively useful because it discourages me from standing in dangerous locations (such as cliff edges) where I might be likely to fall and hurt myself.</p></li>
</ul>
<p>Let’s revisit the last part of the APA’s definition of emotion: “by which an individual attempts to deal with a personally significant matter or event”. This part of the definition is particularly controversial in the context of music emotion research, because the emotions that music induces are generally <em>not</em> to do with any personally significant matter or event. Some people respond by saying that the definition is at fault, in that it is overly restrictive. Others respond by saying that musical emotions aren’t ‘real’ emotions, precisely because they fail to satisfy all of these criteria.</p>
<p>This debate on the validity of musical emotions has a long philosophical history which we won’t dive into here. However, we will suggest a slightly modified and more inclusive definition of emotion that sidesteps this controversy: “Emotion is a complex reaction pattern, involving experiential, behavioural, and physiological elements, typically (but not exclusively) elicited as a response to a personally significant matter or event.”</p>
</div>
<div id="mood" class="section level4">
<h4>
<span class="header-section-number">15.1.1.2</span> Mood<a class="anchor" aria-label="anchor" href="#mood"><i class="fas fa-link"></i></a>
</h4>
<p>Mood is a related concept to emotion. The APA Dictionary of Psychology defines mood as “a disposition to respond emotionally in a particular way that may last for hours, days, or even weeks, perhaps at a low level and without the person knowing what prompted the state.” In other words, the key differences between emotions and moods would be:</p>
<ul>
<li><p>Emotions typically have short duration, whereas moods have long duration;</p></li>
<li><p>Emotions typically have moderate to high intensity, whereas moods have low intensity;</p></li>
<li><p>Emotions typically have obvious causes that are available to conscious introspection, whereas moods do not necessarily have obvious causes, at least from the perspective of the person who’s having the mood.</p></li>
</ul>
</div>
<div id="feeling" class="section level4">
<h4>
<span class="header-section-number">15.1.1.3</span> Feeling<a class="anchor" aria-label="anchor" href="#feeling"><i class="fas fa-link"></i></a>
</h4>
<p>We can define ‘feeling’ as “the subjective experience of an emotion or mood”. In the context of emotions, the ‘feeling’ corresponds to the experiential component of the three-part response pattern we described earlier.</p>
</div>
<div id="affect" class="section level4">
<h4>
<span class="header-section-number">15.1.1.4</span> Affect<a class="anchor" aria-label="anchor" href="#affect"><i class="fas fa-link"></i></a>
</h4>
<p>We can then define ‘affect’ as an umbrella term that encompasses emotions, moods, and feelings. This is a useful term to use when we don’t want to commit to one of these finer categories.</p>
</div>
<div id="perceived-versus-felt-affect" class="section level4">
<h4>
<span class="header-section-number">15.1.1.5</span> Perceived versus felt affect<a class="anchor" aria-label="anchor" href="#perceived-versus-felt-affect"><i class="fas fa-link"></i></a>
</h4>
<p>The distinction between <em>perceived</em> and <em>felt</em> affect is a recurring one in the music emotion literature. We will discuss both terms now.</p>
<div id="perception-and-expression" class="section level5">
<h5>
<span class="header-section-number">15.1.1.5.1</span> Perception and expression<a class="anchor" aria-label="anchor" href="#perception-and-expression"><i class="fas fa-link"></i></a>
</h5>
<p>When we say that someone ‘perceives’ an affect, we mean that they are recognising a potential signal for that affect. This signal could be embodied in all kinds of ways, for example in the prosody of a speech utterance, or in the tonal structure of a melody. For example, most Western listeners will perceive ‘Old MacDonald’ as being a happy melody.</p>
<p>The creative counterpart of perception is expression. When we say that someone ‘expresses’ an affect, we mean that they are creating a signal for the affect that could in theory be perceived by someone else.</p>
</div>
<div id="feeling-and-induction" class="section level5">
<h5>
<span class="header-section-number">15.1.1.5.2</span> Feeling and induction<a class="anchor" aria-label="anchor" href="#feeling-and-induction"><i class="fas fa-link"></i></a>
</h5>
<p>When we say that someone ‘feels’ an affect, we mean that they actually experience that affect. Feeling an affect is considered to be a stronger process than simply perceiving an effect. For example, I might hear a minor-mode melody and recognise that it is expressing sadness, but I may not experience that sadness myself in any concrete way. Feeling an affect might be just an experiential phenomenon, but it might also include behavioural and physiological components.</p>
<p>The creative counterpart of feeling is induction. When we say that a stimulus ‘induces’ an affect, we mean that the stimulus causes the participant to feel that effect. Induction is a stronger process than expression.</p>
</div>
</div>
</div>
</div>
<div id="modelling-emotion" class="section level2">
<h2>
<span class="header-section-number">15.2</span> Modelling emotion<a class="anchor" aria-label="anchor" href="#modelling-emotion"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous section we ended up with the following definition of emotion: “Emotion is a complex reaction pattern, involving experiential, behavioural, and physiological elements, typically (but not exclusively) elicited as a response to a personally significant matter or event.”</p>
<p>It’s possible to imagine an almost infinite number of ways in which an individual can respond to a ‘personally significant matter or event’, especially if we are including all three components of experience, behaviour, and physiology. We need some kind of way to make sense of all of these possible reaction patterns. This is the purpose of emotion models: to organise this space of potential reactions in a meaningful way.</p>
<p>We’re going to cover a few different emotion models in this session. They’re not <em>computational</em> models, in the sense of Chapter <a href="computational-music-psychology.html#computational-music-psychology">28</a> – they’re better described as <em>theoretical</em> models. These models fall into two main categories: <em>dimensional</em> models and <em>categorical</em> models.</p>
<div id="dimensional-models" class="section level3">
<h3>
<span class="header-section-number">15.2.1</span> Dimensional models<a class="anchor" aria-label="anchor" href="#dimensional-models"><i class="fas fa-link"></i></a>
</h3>
<p>Dimensional models express emotions as points in continuous space. The location of a point in continuous space is fully specified by a series of numbers, with one number corresponding to each dimension.</p>
<p>For example, consider this point in 2-dimensional space. It has the coordinates (4, 2). This means it has a score of 4 on dimension 1, and a score of 2 on dimension 2.</p>
<div class="inline-figure"><img src="images/2-dimensions.png" width="300"></div>
<p>These continuous spaces can have arbitrary numbers of dimensions. In this next example we have three dimensions. Our point is located at coordinates (4, 2, 3), corresponding to scores of 4, 2, and 3 on the three dimensions respectively.</p>
<div class="inline-figure"><img src="images/3-dimensions.png" width="300"></div>
<p>The best-known dimensional model of emotions is the <em>circumplex</em> model, introduced by Russell in 1980 <span class="citation">(Russell, <a href="#ref-russell1980" role="doc-biblioref">1980</a>)</span>. This is a very simple model, with just two dimensions. It works surprisingly well nonetheless.</p>
<p>The first dimension is the <em>arousal</em> dimension. Emotions can vary in a continuous way between low arousal and high arousal.</p>
<p>Arousal is associated with subjective energy levels. When in a state of high arousal, we feel awake, activated, and reactive to stimuli. This subjective feeling is coupled with a variety of distinctive physiological features, including increased heart rate, increased blood pressure, increased perspiration (i.e. sweating), increased respiration (i.e. breathing) rate, increased muscle tension, and increased metabolic rate. Together, these different physiological features serve the function of preparing the individual to perform some kind of physical or mental task in the near future.</p>
<p>The second dimension is the <em>valence</em> dimension. Emotions can vary on a continuous scale between very negative valence and very positive valence. Positive valence corresponds to what we might call ‘good’ feelings. Positive valence is a desirable state to be in. In contrast, negative valence corresponds to bad feelings, and an undesirable state. In an emotional context, the primary function of valence is presumably to encourage the individual to take decisions that are good for the organism’s security, longevity, and reproductive success.</p>
<p>If we put these two dimensions together, we get the so-called circumplex model of emotion:</p>
<div class="inline-figure"><img src="images/circumplex.png" width="300"></div>
<p>The idea then is that an individual’s emotional state at a particular point in time can be represented in terms of their locations on these two dimensions of arousal and valence.</p>
<ul>
<li><p>Imagine a footballer who just scored a goal, and is performing a victory celebration: we’d expect them to be experiencing <strong>high arousal</strong> and <strong>positive valence</strong>.</p></li>
<li><p>If I’m running to catch a train, I’m probably in a state of <strong>high arousal</strong> but <strong>negative valence</strong>. The arousal helps me to run faster, whereas the low valence discourages me from cutting it so fine next time.</p></li>
<li><p>If I’m bored at work, that’s probably a state of <strong>low arousal</strong> and <strong>negative valence</strong>.</p></li>
<li><p>Lastly, if I’m cozy in bed, I’d expect both <strong>low arousal</strong> and <strong>positive valence</strong>.</p></li>
</ul>
<p>While the circumplex model works fairly well with just two dimensions, we can get further by adding more dimensions. One example is the ‘Pleasure, Arousal, Dominance’ model, which we can abbreviate to the ‘PAD’ model <span class="citation">(Mehrabian &amp; Russell, <a href="#ref-mehrabian1974approach" role="doc-biblioref">1974</a>)</span>. Two of the PAD model’s dimensions correspond closely to dimensions of the circumplex model: the PAD model’s Pleasure-Displeasure dimension corresponds closely to the circumplex model’s valence dimension, and the PAD model’s Arousal-Nonarousal dimension corresponds to the circumplex model’s arousal dimension. The main difference is the new third dimension, the Dominance-Submissiveness dimension.</p>
<p>This new Dominance-Submissiveness dimension helps us to distinguish certain important emotions that were indistinguishable under the circumplex model. Take anger and fear, for example: they are both essentially equivalent under the circumplex model, because they both corresponded to negative valence and high arousal. The new Dominance-Submissiveness dimension distinguishes both these emotions: anger corresponds to dominance, whereas fear corresponds to submissiveness.</p>
</div>
<div id="categorical-models" class="section level3">
<h3>
<span class="header-section-number">15.2.2</span> Categorical models<a class="anchor" aria-label="anchor" href="#categorical-models"><i class="fas fa-link"></i></a>
</h3>
<p>As implied by the name, categorical models organise the space of all possible emotions into a set of discrete categories, each with its own special set of identifying characteristics.</p>
<p>The English language already provides many different words to describe emotion-related concepts. If we wanted, we could treat each word as its own emotional category, and this would make sense in a certain way: words are rarely ever exact synonyms, and each one will provide its own particular set of connotations and implications. However, this wouldn’t be all that useful for helping us to understand the psychological nature of emotion: we’d have many, many different categories, and no good idea of which ones are more important or fundamental than others.</p>
<p>One useful concept here is Ekman’s concept of basic emotions <span class="citation">(Ekman, <a href="#ref-ekman1999" role="doc-biblioref">1999</a>)</span>. Ekman’s argument is that we should understand the psychology of emotions in terms of a set of so-called ‘basic emotions’, which are the building blocks for all of human emotional experience.</p>
<p>Ekman specified a strict set of requirements for something to count as a basic emotion. These are specified in detail in <span class="citation">Ekman (<a href="#ref-ekman1999" role="doc-biblioref">1999</a>)</span>:</p>
<ul>
<li><p>First, it must have a set of distinctive <em>universal signals</em>; these signals function to communicate one’s emotional state with conspecifics. According to Ekman, these signals should be shared cross-culturally.</p></li>
<li><p>Second, the emotion must have a distinctive <em>physiological signature</em>, encompassing variables such as heart rate and perspiration.</p></li>
<li><p>Third, the emotion must possess an automatic <em>appraisal mechanism</em>: in other words, the emotion must arise spontaneously as a reaction to a situation, rather than being the consequence of careful and considered thought.</p></li>
</ul>
<p>Lastly, the emotion must possess universal <em>antecedent events</em>: that means that the same kinds of events should elicit the same emotions cross-culturally. For example, we might suppose that rotten food always elicits disgust.</p>
<p>To take one example, these definitions rule out ‘nostalgia’ as a basic emotion. Nostalgia does not have a clear set of distinctive universal signals, or a clear set of associated physiological manifestations.</p>
<p>Ekman identified a core set of emotions that do fulfil these basic criteria. These are: anger, disgust, fear, happiness, contempt, sadness, and surprise. It’s worth going through each of these and thinking about how humans signal these emotions, how our behaviour is influenced by these emotions, and how our body reacts physiologically to them.</p>
<p>Ekman also identified a collection of other emotions that might one day be proved to be basic emotions. These candidates include things like sensory pleasures, amusement, relief, and excitement.</p>
<p>So, to summarise Ekman’s perspective: he’s claiming that we should be careful about what we call an ‘emotion’. It’s best to focus on ‘core’ or ‘basic’ emotions that are universal across the human species, and that have distinctive manifestations in terms of subjective experience, physiology, and signalling.</p>
</div>
<div id="dimensional-versus-categorical-models" class="section level3">
<h3>
<span class="header-section-number">15.2.3</span> Dimensional versus categorical models<a class="anchor" aria-label="anchor" href="#dimensional-versus-categorical-models"><i class="fas fa-link"></i></a>
</h3>
<p>Both dimensional and categorical approaches have their own advantages and disadvantages. This makes different approaches useful in different situations.</p>
<p>The dimensional models provide particularly efficient summaries of relationships between emotions. This is very useful in helping us to organise the great diversity of potential emotional experiences in an understandable way. If we’re lucky, the underlying dimensions for these models can also have biological significance, in corresponding to underlying physiological mechanisms. The arousal dimension of the circumplex and PAD models is a good example of this, corresponding to a collection of clearly identifiable physiological variables.</p>
<p>The main limitation of the dimensional models is their limited expressivity. It is often quite easy to think of pairs of emotions that have clearly distinct response patterns, but have similar representations under a given dimensional model. So, in themselves, dimensional models struggle to provide a complete account of human emotion.</p>
<p>Categorical emotions address this problem quite well. They are well-suited to capturing subtle differences between emotions: any time you want to differentiate two emotions, you can just create a new category.</p>
<p>However, this flexibility is also the weakness of this approach. It’s easy to create more and more categories to capture various nuances of emotion, and eventually you end up with a long ‘shopping list’ of many different emotion terms, with no clear structure behind it. In order to understand how these terms fit together, you then have to start thinking about a supplementary organisation scheme, for example organising them into a hierarchical taxonomy, or overlaying them onto a dimensional model and so on. So, the two approaches can work quite well together.</p>
<p>There is still an important underlying psychological question here, and that is whether human emotions are fundamentally organised into categories or not, or whether the space of possible emotions is fundamentally continuous. This question is still being debated in the literature, and there is still no clear answer. Unfortunately, the conclusion seems to depend a lot on how you define emotion…</p>
</div>
<div id="aesthetic-and-musical-emotions" class="section level3">
<h3>
<span class="header-section-number">15.2.4</span> Aesthetic and musical emotions<a class="anchor" aria-label="anchor" href="#aesthetic-and-musical-emotions"><i class="fas fa-link"></i></a>
</h3>
<p>We’re now going to move from Ekman’s basic emotions model onto emotion models that specifically concern aesthetic and musical emotions. These models typically take a much more relaxed approach to deciding what might constitute an emotion. In particular, many of the emotions being described in this context don’t have any well-established physiological correlates or universal signals.</p>
<div id="geneva-musical-emotions-scale-gems" class="section level4">
<h4>
<span class="header-section-number">15.2.4.1</span> Geneva Musical Emotions Scale (GEMS)<a class="anchor" aria-label="anchor" href="#geneva-musical-emotions-scale-gems"><i class="fas fa-link"></i></a>
</h4>
<p>The Geneva Emotional Music Scale (GEMS) was introduced by <span class="citation">Zentner et al. (<a href="#ref-Zentner2008-mv" role="doc-biblioref">2008</a>)</span>. Compared to the ‘basic emotions’ model discussed earlier, the GEMS has a much more inclusive definition of emotions. Instead of working under strict definitions of what features are needed to make something an emotion, the authors instead compiled a very broad set of terms from the literature and from other sources, and used a data-driven approach to work out which terms were most relevant for musical applications.</p>
<p>The way this worked is that the authors conducted a series of experiments based on self-report questionnaires, designed to probe how people experience emotions in musical contexts. These experiments were centred on two main questions. First, what emotions are most commonly induced by musical experiences? Secondly, what emotions tend to co-occur with each other in these musical experiences?</p>
<p>The output of this work was the following emotion model:</p>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="images/GEMS.svg" alt="The factorial structure of the GEMS, redrawn after @Zentner2008-mv." width="100%"><p class="caption">
Figure 15.1: The factorial structure of the GEMS, redrawn after <span class="citation">Zentner et al. (<a href="#ref-Zentner2008-mv" role="doc-biblioref">2008</a>)</span>.
</p>
</div>
<p>The model is hierarchical, in that it describes musical emotions in terms of three nested layers. The top layer has just three categories: sublimity, vitality, and unease. The second layer splits these categories into subcategories: for example, unease is split into tension and sadness, whereas vitality is split into power and joyful activation. Each of these subcategories is then split into more granular terms: for example, sadness is split into ‘sad’ and ‘sorrowful’. This nested structure gives us an idea about how different kinds of emotional experiences tend to go together in music listening contexts.</p>
<p>One interesting thing to see from this model is a clear <em>positivity</em> bias. The most commonly reported emotions are positive ones, such as wonder, transcendence, and tenderness: negative emotions, such as tension and sadness, play a comparatively small role.</p>
<p>The analysis approach of the GEMS identifies certain ‘factors’ corresponding to emotion terms that often go together in musical experiences. For example, the makeup of the ‘Tension’ term tells us that the terms ‘agitated’, ‘nervous’, ‘tense’, ‘impatient’, and ‘irritated’ tend to occur in conjunction when people talk about musical experiences. This doesn’t necessarily tell us though that there is an underlying unitary emotion called ‘tension’, certainly not according to Ekman’s definition of basic emotions. However, this co-occurrence does suggest a meaningful connection between these different emotional concepts in terms of the musical features that elicit them. These connections could be probed in more detail in experimental studies.</p>
</div>
<div id="aesthetic-emotions" class="section level4">
<h4>
<span class="header-section-number">15.2.4.2</span> Aesthetic emotions<a class="anchor" aria-label="anchor" href="#aesthetic-emotions"><i class="fas fa-link"></i></a>
</h4>
<p><em>Aesthetic emotions</em> concern the perception of beauty in a wider context: not just music, but also artworks, landscapes, architecture, and so on.</p>
<p><span class="citation">Menninghaus et al. (<a href="#ref-menninghaus2019" role="doc-biblioref">2019</a>)</span> have presented a detailed case for the notion of aesthetic emotions. Their conceptualisation organises aesthetic emotions into two main classes.</p>
<p>The first class corresponds to aesthetic versions of ‘ordinary’ emotions. These emotions correspond to an ‘ordinary’ emotion, such as joy, amusement, or nostalgia, or surprise, that an aesthetic object can induce in us.</p>
<p>The second category contains emotions that concern the appreciation of aesthetic virtues. This category might include emotions such as ‘the feeling of beauty’, ‘the feeling of sublime’, ‘the feeling of groove’, and so on.</p>
<p>The authors make various important observations about aesthetic emotions:</p>
<ul>
<li><p><strong>Aesthetic emotions are deeply linked with pleasure and displeasure</strong>. They therefore make an important contribution to the overall attractiveness of an aesthetic experience.</p></li>
<li><p><strong>Pleasure is largely determined by the intensity of the aesthetic emotion</strong>. So, we can experience pleasure on account of an intense feeling of joy, of amusement, of beauty, of sublimity, and so on.</p></li>
<li><p><strong>An intense experience can be pleasant</strong>, even for an aesthetic emotion that corresponds to a ‘negative’ ordinary emotion. For example, if music evokes a strong feeling of sadness, this will generally still correspond to a pleasant aesthetic experience.</p></li>
</ul>
<p>The authors also note that aesthetic emotions possess important and necessary characteristics of ‘ordinary’ emotions. They are associated with clear physiological variables, such as heart rate, perspiration, and chills; they are associated with clear signals, such as laughter, tears, and smiling; they’re also associated with clear behavioural outcomes, such as a tendency to end, extend or repeat exposure to the aesthetic object. That said, it’s important to note that these features are not necessarily ‘distinctive’ in the way required by Ekman’s definitions of basic emotions; most of the time, they seem to co-opt response patterns from pre-existing basic emotions, such as happiness and sadness.</p>
<p>This leads us onto an important open question in the literature: do we really need to posit aesthetic emotions as a separate category of emotion? There is an interesting series of response articles in the literature debating this point. The skeptic’s position (e.g. <span class="citation">(Skov &amp; Nadal, <a href="#ref-skov2020" role="doc-biblioref">2020</a>)</span>) is that aesthetic emotions are best understood in terms of ‘ordinary’ emotions. These emotions might be experienced in a special combination, perhaps even a combination that is difficult to achieve in normal life (e.g. anger and happiness). They might also be experienced in an unusual presentation: for example, experiencing sadness in an aesthetic context might remove its behavioural components (which typically make sadness an unpleasant experience) while leaving other components of physiology, signalling, and subjective experience unaffected. This is all still a topic of debate, and there’s no clear resolution in sight!</p>
</div>
</div>
<div id="conclusions-1" class="section level3">
<h3>
<span class="header-section-number">15.2.5</span> Conclusions<a class="anchor" aria-label="anchor" href="#conclusions-1"><i class="fas fa-link"></i></a>
</h3>
<p>Emotion is something of a paradoxical topic to study psychologically. On the one hand, as humans we are all intuitively aware of what it means to experience emotions, in a way that arguably can never be expressed fully in words. This can make a lot of this topic feel trivial, in a certain way. On the other hand, when we dig deeper, we realise that there are some very deep issues here that deserve careful examination.</p>
<p>The models we talked about today – dimensional and categorical models – can help us to structure these investigations. We talked about two particular examples of dimensional models: the two-dimensional circumplex model, and the three-dimensional PAD model. We then talked about three categorical approaches to emotion: the basic emotions theory <span class="citation">(Ekman, <a href="#ref-ekman1999" role="doc-biblioref">1999</a>)</span>, the GEMS musical emotions model <span class="citation">(Zentner et al., <a href="#ref-Zentner2008-mv" role="doc-biblioref">2008</a>)</span>, and the aesthetic emotions model of <span class="citation">Menninghaus et al. (<a href="#ref-menninghaus2019" role="doc-biblioref">2019</a>)</span>. They make for quite interesting comparisons. We’re still waiting though for a proper combined model that convincingly accounts for all of these phenomena together.</p>
</div>
</div>
<div id="measuring-emotional-responses" class="section level2">
<h2>
<span class="header-section-number">15.3</span> Measuring emotional responses<a class="anchor" aria-label="anchor" href="#measuring-emotional-responses"><i class="fas fa-link"></i></a>
</h2>
<p>This section introduces various ways that we can measure emotional responses in the context of psychological experiments. Broadly speaking, we can organise these measurement techniques into three primary categories: self-report measures, physiological measures, and neural measures.</p>
<div id="self-report" class="section level3">
<h3>
<span class="header-section-number">15.3.1</span> Self-report<a class="anchor" aria-label="anchor" href="#self-report"><i class="fas fa-link"></i></a>
</h3>
<p>The idea behind self-report measures is simply to ask the participant about their emotional experiences. This relies on the fact that, at least in Western society, most adult participants have fairly sophisticated vocabularies for describing their emotional experiences.</p>
<p>There are many ways in which we could ask someone about their emotional experiences, in more or less structured ways. A particularly common approach though is to use rating scales; here we simply ask people to describe an emotional experience by rating it according to various criteria, for example ‘joyfulness’ or ‘anger’. These numbers can then be aggregated to provide a quantitative summary of the participant’s emotional responses.</p>
<p>There are various kinds of rating scales we could use here. One prominent option is the Geneva Emotional Music Scale, or ‘GEMS’ (Zentner et al., 2008), which we discuss in Section <a href="emotion.html#modelling-emotion">15.2</a>.</p>
<p>The GEMS takes many different emotion terms (on the left hand side here), and clusters them together into a three-level hierarchy. The scale was constructed in a data-driven way: the emotion terms are terms that frequently occur when people describe music-induced emotional experiences, and the higher-level clusters (or ‘factors’) describe how these different emotion terms tend to co-occur with each other.</p>
<p>We can use the GEMS to make questionnaires for quantifying certain kinds of core emotional responses to music. For example, suppose I want to quantify the tension that someone experiences when they listen to a particular piece of music. The GEMS tells me that a good way to do that is to give the participant rating scales for five adjectives: agitated, nervous, tense, impatient, and irritated. I get the participant to rate their experience on these five adjectives. I then combine their results, for example by averaging, and this gives me an overall ‘tension’ score. Similarly, the GEMS tells me that I could compute scores for a general ‘unease’ factor by first computing scores for ‘tension’ and ‘sadness’, and then combining them.</p>
<div class="figure">
<img src="images/gems-subset.png" width="600" alt=""><p class="caption">The ‘Unease’ component of the GEMS, redrawn after <span class="citation">Zentner et al. (<a href="#ref-Zentner2008-mv" role="doc-biblioref">2008</a>)</span>.</p>
</div>
<p>This kind of approach has several advantages. It’s easy to get data for very specific adjectives: if I’m interested in nostalgia, I can simply ask the participant ‘to what extent does this music make you feel nostalgic’? The approach is also rather efficient – it only takes a moment for the participant to record what emotion they feel. There’s also no need for expensive laboratory equipment, all you need is pen and paper.</p>
<p>However, there are some important disadvantages. The results are mediated by the participant’s vocabulary, and this is problematic for working with people with limited vocabularies, as well as for generalising the paradigm across different languages or cultures. There’s also a problem that participants can easily get confused between the two concepts of ‘expressed’ and ‘induced’ emotion. In a particular study, we might be specifically interested in what kinds of emotions the music makes the participant <em>feel</em>, but if we ask the participant to describe what emotions the music makes them feel, it’s quite common for the participant to mistakenly describe what emotions the music seems to be expressing.</p>
</div>
<div id="physiological" class="section level3">
<h3>
<span class="header-section-number">15.3.2</span> Physiological<a class="anchor" aria-label="anchor" href="#physiological"><i class="fas fa-link"></i></a>
</h3>
<div id="heart-rate" class="section level4">
<h4>
<span class="header-section-number">15.3.2.1</span> Heart rate<a class="anchor" aria-label="anchor" href="#heart-rate"><i class="fas fa-link"></i></a>
</h4>
<p>An obvious thing to measure is the heart rate; this can be achieved with inexpensive and unobtrusive heart rate monitors. Many smart watches nowadays even include heart rate monitors as standard. Two aspects of heart rate are typically monitored: the absolute value and the variability. The arousal component of emotion is typically associated both with high absolute values and high variability in values, reflecting the body’s preparation for imminent physical exertion.</p>
</div>
<div id="respiration-rate" class="section level4">
<h4>
<span class="header-section-number">15.3.2.2</span> Respiration rate<a class="anchor" aria-label="anchor" href="#respiration-rate"><i class="fas fa-link"></i></a>
</h4>
<p>The respiration rate corresponds to the number of breaths that the participant takes per minute. One way of measuring this is with a ‘pneumograph’, as displayed here: a pneumograph measures respiration by tracking chest movements. High respiration rates are associated with high arousal.</p>
<div class="figure">
<img src="images/pneumograph.png" width="300" alt=""><p class="caption">Credit: <a href="https://commons.wikimedia.org/wiki/File:Man_and_abnormal_man,_including_a_study_of_children,_in_connection_with_bills_to_establish_laboratories_under_federal_and_state_governments_for_the_study_of_the_criminal,_pauper,_and_defective_(14590690799).jpg">Internet Archive Book Images</a>, no restrictions, via Wikimedia Commons</p>
</div>
</div>
<div id="goosebump-recorder" class="section level4">
<h4>
<span class="header-section-number">15.3.2.3</span> Goosebump recorder<a class="anchor" aria-label="anchor" href="#goosebump-recorder"><i class="fas fa-link"></i></a>
</h4>
<p>Intense emotional experiences, especially in the context of music, are often accompanied by skin tingling and goosebumps, corresponding to a process called ‘piloerection’ where body hairs stand on end. These experiences are called ‘chills’. It’s possible to record the development of goosebumps using custom cameras placed for example on the participant’s arm.</p>
<div class="figure">
<img src="images/piloerection.gif" width="300" alt=""><p class="caption">Schematic illustration of piloerection. Credit: <a href="https://commons.wikimedia.org/wiki/File:PilioerectionAnimation.gif">AnthonyCaccese</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons</p>
</div>
</div>
<div id="skin-conductance-response" class="section level4">
<h4>
<span class="header-section-number">15.3.2.4</span> Skin conductance response<a class="anchor" aria-label="anchor" href="#skin-conductance-response"><i class="fas fa-link"></i></a>
</h4>
<p>Increased arousal is associated with increased perspiration, or sweating. It’s possible to quantify this perspiration effect by measuring the so-called ‘skin conductance response’. Here we place two electrodes on the skin, and pass a small current between them. The ease with which this current travels depends on the skin’s conductance, and this conductance depends on perspiration levels: increased levels of perspiration correspond to greater levels of conductance.</p>
</div>
<div id="skin-temperature" class="section level4">
<h4>
<span class="header-section-number">15.3.2.5</span> Skin temperature<a class="anchor" aria-label="anchor" href="#skin-temperature"><i class="fas fa-link"></i></a>
</h4>
<p>Skin temperature provides another proxy for arousal. We can measure skin temperature with an infrared camera, for example. Increased arousal tends to be reflected in a drop in nasal skin temperature, as a result of restricted blood flow in peripheral regions.</p>
<div class="figure">
<img src="images/skin-temperature.png" width="350" alt=""><p class="caption">Credit: Clay-Warner, J., &amp; Robinson, D. T. (2015). Infrared thermography as a measure of emotion response. Emotion Review, 7(2), 157-162.</p>
</div>
</div>
<div id="facial-electromyography" class="section level4">
<h4>
<span class="header-section-number">15.3.2.6</span> Facial electromyography<a class="anchor" aria-label="anchor" href="#facial-electromyography"><i class="fas fa-link"></i></a>
</h4>
<p>Facial electromyography is based on the observation that certain emotions tend to elicit certain universal facial expressions, for example smiling or frowning. Often the participant will not smile or frown in a significant enough way for this smile or frown to be detected by an external observer; however, if we measure muscle activity in certain facial muscles, we may still be able to detect some signs of stimulation. This is what facial electromyography is for: we place electrodes next to certain muscles in the face, and record electrical activity at the locations of these muscles. For example, we can place electrodes on the zygomaticus major muscle, and this can give us a proxy of smiling. We can also place electrodes on the corrugator supercilii muscle, which is associated with frowning.</p>
<div class="figure">
<img src="images/zygomaticus-major.png" width="500" alt=""><p class="caption">Credit: Uwe Gille, Public domain, via Wikimedia Commons</p>
</div>
<div class="figure">
<img src="images/zygomaticus-minor.png" width="500" alt=""><p class="caption">Credit: Uwe Gille, Public domain, via Wikimedia Commons</p>
</div>
<p>Unfortunately, this can be quite an invasive procedure for the participant, with their face ending up being covered in electrodes. In practice, it can be preferable to use a less invasive method of facial monitoring instead, such as a video camera, sacrificing some sensitivity for the sake of a more naturalistic listening experience.</p>
</div>
<div id="pupillometry" class="section level4">
<h4>
<span class="header-section-number">15.3.2.7</span> Pupillometry<a class="anchor" aria-label="anchor" href="#pupillometry"><i class="fas fa-link"></i></a>
</h4>
<p>Pupillometry can also provide an insight into a participant’s emotional responses. Normally, the pupil dilates (i.e. expands) and contracts depending on light levels, but pupil dilation also occurs in the context of arousal and surprise. We can record these dilation responses by focusing a camera on the participant’s eye and recording their pupil size over time.</p>
<div class="figure">
<img src="images/pupil.jpg" width="250" alt=""><p class="caption">Credit: Petr Novák, Wikipedia, <a href="https://creativecommons.org/licenses/by-sa/2.5">CC BY-SA 2.5</a></p>
</div>
</div>
<div id="conclusions-2" class="section level4">
<h4>
<span class="header-section-number">15.3.2.8</span> Conclusions<a class="anchor" aria-label="anchor" href="#conclusions-2"><i class="fas fa-link"></i></a>
</h4>
<p>These physiological measures have certain important advantages. One advantage is we avoid having our results mediated through vocabulary; this means that we can easily apply the same methods to many different populations, including cross-cultural populations or linguistically impaired populations. A second advantage is that these measures are by definition necessary if we want to understand the full response pattern underlying a particular emotion. As we discussed before, the definition of an emotion typically includes a physiological component, and we must characterise this physiological component properly if we want to understand the emotion.</p>
<p>Unfortunately, physiological measures come with their own disadvantages. They tend to produces rather noisy data streams, which means in practice that we need more participants or longer testing sessions in order to reach sufficient reliability levels. It’s also rather difficult to distinguish emotions in fine detail using these methods. If we want to distinguish emotions that vary solely on arousal, these methods can work pretty well, but other kinds of distinctions are harder to make. Lastly, the methods can be invasive to a certain degree. Anything that involves placing a physical device on the participant will introduce some distraction and potentially alter the nature of the listening experience.</p>
</div>
</div>
<div id="neuroimaging" class="section level3">
<h3>
<span class="header-section-number">15.3.3</span> Neuroimaging<a class="anchor" aria-label="anchor" href="#neuroimaging"><i class="fas fa-link"></i></a>
</h3>
<p>The purpose of neural measures is to get some insight into the brain processing underlying a particular emotional response.</p>
<p>There are many different neural measures out there, but here we will focus in particular on <em>functional</em> <em>magnetic resonance imaging</em>, or fMRI. This method is particularly attractive for emotion studies because, unlike several other methods, it is well-suited to accessing the deeper brain regions that tend to be associated with emotion processing. The method is also particularly good at providing precise spatial localisation of activity within the brain, which is helpful for differentiating the precise processes that are going on.</p>
<p>fMRI assesses blood oxygen levels in different regions of the brain, which provide a relatively real-time marker of ongoing neural activity. By looking at the locations of this neural activity, we can hypothesise about what brain areas are involved in a particular cognitive process.</p>
<div class="figure">
<img src="images/fmri.jpg" width="300" alt=""><p class="caption">Credit: Daniel Bell, Frank Gaillard, et al, Radiopaedia.org. <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/no/">CC-BY-NC-ND 3.0</a></p>
</div>
<p>Through this process, neuroscientists have identified various regions of the brain involved in various aspects of emotional responses to music. One such region is the <em>amygdala</em> – this is implicated in both pleasant and unpleasant emotions, particularly fear, anxiety, and aggression. It’s thought to be involved in coding music’s affective positivity or negativity.</p>
<div class="figure">
<img src="images/amygdala.jpg" width="450" alt=""><p class="caption">Credit: <a href="https://commons.wikimedia.org/wiki/File:Gray_718-amygdala.png">Henry Vandyke Carter</a>, public domain, via Wikimedia Commons</p>
</div>
<p>The <em>nucleus accumbens,</em> which is part of the ventral striatum, seems to be another important region in explaining emotional responses to music. In general psychological contexts, the nucleus accumbens has been implicated in motivation, aversion, and reward. In the context of music, the nucleus accumbens seems to be particularly involved in the generation of pleasure.</p>
<div class="figure">
<img src="images/nucleus-accumbens.png" width="400" alt=""><p class="caption">Credit: <a href="https://commons.wikimedia.org/wiki/File:Nucleus_accumbens.svg">Leevanjackson</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons</p>
</div>
<p>Lastly, there’s the <em>hippocampus</em>. The hippocampus seems to have many different functions, but in the context of general emotion research it has been linked to tender positive emotions and behaviours such as joy, love, compassion, empathy, grooming, and nursing offspring. This seems to translate over to music emotion studies, which have found the hippocampus to be involved in various music-evoked emotions including tenderness, peacefulness, joy, and sadness.</p>
<div class="figure">
<img src="images/hippocampus.jpg" width="400" alt=""><p class="caption">Credit: <a href="https://commons.wikimedia.org/wiki/File:Gray739-emphasizing-hippocampus.png">Henry Vandyke Carter</a>, public domain, via Wikimedia Commons</p>
</div>
<p>These kinds of neural measures have a very important advantage over the other methods, which is that they can identify specific brain regions involved in particular experimental contexts.</p>
<p>They do however have some important disadvantages. One is that they involve expensive equipment – an fMRI machine will cost many hundreds of thousands of pounds, and is expensive to maintain. fMRI machines in particular tend to be rather noisy, which can be problematic in music experiments for obvious reasons. The final issue is perhaps the deepest, and concerns the interpretation of the results. Any particular brain region will typically have multiple functions, and so decoding patterns of brain activations is a very complex task. It’s not possible right now to look at an fMRI scan and say ‘aha, this person is experiencing nostalgia…’.</p>
</div>
<div id="conclusions-3" class="section level3">
<h3>
<span class="header-section-number">15.3.4</span> Conclusions<a class="anchor" aria-label="anchor" href="#conclusions-3"><i class="fas fa-link"></i></a>
</h3>
<p>Here we covered three different approaches to measuring emotions that could be applied in music studies: self-report measures, physiological measures, and neural measures. These methods all bring their own strengths and weaknesses, and we arguably need all of them if we want to fully characterise an emotional response. In practice though, we’re limited by the available resources. Self-report measures end up being particularly popular as a result!</p>
</div>
</div>
<div id="mechanisms-for-inducing-musical-emotion" class="section level2">
<h2>
<span class="header-section-number">15.4</span> Mechanisms for inducing musical emotion<a class="anchor" aria-label="anchor" href="#mechanisms-for-inducing-musical-emotion"><i class="fas fa-link"></i></a>
</h2>
<p>This section is about different mechanisms by which music seems to induce emotions. We’re going to focus on a particularly influential theoretical model by Juslin, called the <strong>‘BRECVEM’</strong> model. This model is described in a 2008 paper in Behavioral and Brain Sciences by <span class="citation">Juslin &amp; Västfjäll (<a href="#ref-juslin2008" role="doc-biblioref">2008</a>)</span>. This is a really good paper to read if you have the time. As well as providing a detailed and well-organised exposition of the model, the paper also includes a large number of commentaries by various other researchers in the field. This gives you an insight into various complementary perspectives on the topic, and can really help you to develop your own personal opinion about the ideas.</p>
<p>A few years later, Juslin wrote a paper that extends the BRECVEM model to add an additional component, concerning aesthetic judgments <span class="citation">(Juslin, <a href="#ref-juslin2013" role="doc-biblioref">2013</a>)</span>. The result is then called the BRECVEMA model. It’s this particular version of the model that we’ll focus on today. I’d still recommend keeping the original BRECVEM paper as a primary reference though, because it contains particularly detailed expositions of the first seven components of the model.</p>
<p>There are eight components of the full BRECVEMA model: brain stem reflex, rhythmic entrainment, evaluative conditioning, emotional contagion, visual imagery, episodic memory, musical expectancy, and aesthetic judgment. Let’s consider each in turn.</p>
<div id="brain-stem-reflex" class="section level3">
<h3>
<span class="header-section-number">15.4.1</span> Brain stem reflex<a class="anchor" aria-label="anchor" href="#brain-stem-reflex"><i class="fas fa-link"></i></a>
</h3>
<p>The <em>brain stem</em> is one of the oldest parts of the brain, evolutionarily speaking. It has important responsibilities in regulating cardiac and respiratory function. The brain stem reflex is a hardwired attention response that reflects early stages of auditory processing; it’s triggered by sounds that are ‘surprising’ in terms of basic auditory features. Most commonly this means a sudden and loud sound, especially if this sound feels ‘sharp’ or ‘accelerating’. It’s important to respond quickly to these kinds of sounds because they can reflect some kind of immediate threat. Depending on the context, the brain stem reflex can be experienced either as unpleasant or exciting, and is correspondingly linked to arousal.</p>
</div>
<div id="rhythmic-entrainment" class="section level3">
<h3>
<span class="header-section-number">15.4.2</span> Rhythmic entrainment<a class="anchor" aria-label="anchor" href="#rhythmic-entrainment"><i class="fas fa-link"></i></a>
</h3>
<p>Rhythmic entrainment is something of an umbrella term that captures many different modes of entrainment, or synchronisation. It could mean perceptual entrainment, the kind that happens naturally and automatically simply by listening to rhythmic music. It could mean motor entrainment, where you might tap along or dance to the beat. It could be physiological entrainment, where your breathing might synchronise with the music. It could be social entrainment, where you synchronise your movements with several others in your group, perhaps by dancing.</p>
<p>These processes of entrainment can have various implications. Rhythmic entrainment often ends up increasing arousal levels and causing pleasure. Entrainment also seems to evoke feelings of connectedness and emotional bonding; this is particularly true when the entrainment occurs in a group context, and involves multiple individuals synchronising with each other.</p>
</div>
<div id="evaluative-conditioning" class="section level3">
<h3>
<span class="header-section-number">15.4.3</span> Evaluative conditioning<a class="anchor" aria-label="anchor" href="#evaluative-conditioning"><i class="fas fa-link"></i></a>
</h3>
<p>Evaluative conditioning describes an effect whereby repeatedly pairing a piece of music with a given emotion ends up causing a listener to automatically associate that music with the emotion in the future. Importantly, this association doesn’t require conscious awareness: the music can make you happy as a result of the association, but you don’t know why you’re happy. An example could be listening to a particular piece of music that you always used to play when you met your best friend.</p>
</div>
<div id="emotional-contagion" class="section level3">
<h3>
<span class="header-section-number">15.4.4</span> Emotional contagion<a class="anchor" aria-label="anchor" href="#emotional-contagion"><i class="fas fa-link"></i></a>
</h3>
<p>Emotional contagion depends on the fact that listeners will automatically recognise certain features in the music that are similar to markers of emotion in speech <span class="citation">(e.g. Juslin &amp; Laukka, <a href="#ref-juslin2003a" role="doc-biblioref">2003</a>)</span>. For example, if I’m feeling miserable, my speech is likely to be slower, with lower pitch variability, and drooping contour; if I’m feeling excited, my speech is likely to be faster, with higher pitch variability, and potentially a rising contour.</p>
<p><span class="citation">Juslin et al. (<a href="#ref-juslin2001a" role="doc-biblioref">2001</a>)</span> describes an interesting idea, termed ‘super-expressive voice theory’, stating that music’s power of emotional contagion comes not just from the fact that music emulates the expressive capacities of the human voice, but from the fact that it far exceeds them, in terms of the potential manipulations of speed, intensity, and timbre. So, music is accessing the parts of the brain originally intended for interpreting speech expression, and sending them into overdrive.</p>
</div>
<div id="visual-imagery" class="section level3">
<h3>
<span class="header-section-number">15.4.5</span> Visual imagery<a class="anchor" aria-label="anchor" href="#visual-imagery"><i class="fas fa-link"></i></a>
</h3>
<p>This part of the model is based on the notion that music can (in certain listeners) conjure up certain visual images, such as beautiful landscapes. This effect depends on metaphorical, cross-modal associations, for example between ascending melodies and upward movement, or between rich textures and rich colours. The listener can then experience emotions that are a consequence of these visual images.</p>
</div>
<div id="episodic-memory" class="section level3">
<h3>
<span class="header-section-number">15.4.6</span> Episodic memory<a class="anchor" aria-label="anchor" href="#episodic-memory"><i class="fas fa-link"></i></a>
</h3>
<p>This effect is sometimes termed the “Darling, they are playing our tune” effect. It occurs when a listener has paired a particular musical piece with a particular personally significant memory, perhaps a particular event like a wedding or a Christmas party. Unlike the evaluative conditioning mechanism we discussed earlier, episodic memory by definition involves conscious recollection of the associated event. Historically speaking, music theorists haven’t seen episodic memory as being particularly musically relevant, but survey data indicates that episodic memory is a very frequent source of music emotion induction in practice.</p>
<p>It’s been noted for a while now that episodic memories (i.e. memories for particular events from one’s life) tend to be particularly strong for events in youth and early adulthood, between about 15 and 25 years of age. A potential reason for this is simply that many self-defining experiences occur at this age, for example moving away from home, going to university, and gaining financial independence.</p>
<p>This effect seems to be reflected in a musical ‘reminiscence bump’, where music that we originally listening to in youth and early adulthood acquires special retrospective significance. Music from this period tends to be better recognised by the listener, more likely to evoke autobiographical memories, and more likely to evoke emotions.</p>
</div>
<div id="musical-expectancy" class="section level3">
<h3>
<span class="header-section-number">15.4.7</span> Musical expectancy<a class="anchor" aria-label="anchor" href="#musical-expectancy"><i class="fas fa-link"></i></a>
</h3>
<p>We talked about musical expectancy (or expectation) already in a previous section. Music listening generates real-time expectations in the listener; these expectations can then be violated, delayed, or confirmed. Simple violations of expectation, such as unexpected loud chords, may be processed by the brain-stem reflex; more sophisticated and style-dependent violations will depend on the listener’s enculturated knowledge about musical styles, as represented in higher-level brain regions. These manipulations of expectation are associated with emotional phenomena such as anxiety, surprise, thrills, and pleasure.</p>
</div>
<div id="aesthetic-judgment" class="section level3">
<h3>
<span class="header-section-number">15.4.8</span> Aesthetic judgment<a class="anchor" aria-label="anchor" href="#aesthetic-judgment"><i class="fas fa-link"></i></a>
</h3>
<p>This part of the model is probably the most controversial; it’s still not totally agreed that aesthetic judgment requires a separate emotion induction mechanism. This relates to the disputed topic of ‘aesthetic emotions’ that we discussed in a previous section: some researchers believe aesthetic emotions exist, but other researchers believe that so-called ‘ordinary’ emotions are all that we need to explain emotional reactions to art.</p>
<p>Below is a schematic diagram from Juslin’s (2013) paper describing his conceptualisation of the aesthetic judgment process. The listener evaluates the music on several aesthetic criteria, listed here under the headings of beauty, skill, novelty, style, message, expression, and emotion. These different headings are unpacked in more detail in the paper. From these features, the listener makes an overall aesthetic judgment of the piece, which can be positive or negative. Different listeners may weight these different features differently; for example, listeners to avant-garde music might value novelty relatively strongly, but value beauty less. An emotional response then develops on the basis of this overall aesthetic judgment: strongly positive aesthetic judgements elicit positive emotions, whereas strongly negative judgments elicit negative emotions.</p>
<div class="figure">
<img src="images/juslin-aesthetic-emotions.png" style="width:100.0%" alt=""><p class="caption">Figure based on <span class="citation">Juslin (<a href="#ref-juslin2013" role="doc-biblioref">2013</a>)</span>.</p>
</div>
<p>Personally, I think this final component of Juslin’s model is rather speculative, and I’m not completely convinced by it yet. I think it provides a good basis for future exploration, though.</p>
</div>
</div>
<div id="conclusions-4" class="section level2">
<h2>
<span class="header-section-number">15.5</span> Conclusions<a class="anchor" aria-label="anchor" href="#conclusions-4"><i class="fas fa-link"></i></a>
</h2>
<p>We’ve now covered all the aspects of the BRECVEMA model of emotion induction. It’s clear that there are many potential mechanisms by which music can induce emotions. This diversity of mechanisms is an important reason why music has such strong emotional effects on listeners – it can activate these emotions in so many different ways.</p>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ekman1999">
<p>Ekman, P. (1999). Basic emotions. In T. Dalgleish &amp; T. Power (Eds.), <em>The handbook of cognition and emotion</em> (pp. 45–60). John Wiley &amp; Sons, Ltd.</p>
</div>
<div id="ref-juslin2013">
<p>Juslin, P. N. (2013). From everyday emotions to aesthetic emotions: Towards a unified theory of musical emotions. <em>Physics of Life Reviews</em>, <em>10</em>(3), 235–266. <a href="https://doi.org/10.1016/j.plrev.2013.05.008">https://doi.org/10.1016/j.plrev.2013.05.008</a></p>
</div>
<div id="ref-juslin2001a">
<p>Juslin, P. N., Juslin, P. N., &amp; Sloboda, J. A. (2001). Communicating emotion in music performance: A review and a theoretical framework. <em>Music and Emotion: Theory and Research</em>, 309–337.</p>
</div>
<div id="ref-juslin2003a">
<p>Juslin, P. N., &amp; Laukka, P. (2003). Communication of emotions in vocal expression and music performance: Different channels, same code? <em>Psychological Bulletin</em>, <em>129</em>(5), 770–814.</p>
</div>
<div id="ref-juslin2008">
<p>Juslin, P. N., &amp; Västfjäll, D. (2008). Emotional responses to music: The need to consider underlying mechanisms. <em>Behavioral and Brain Sciences</em>, <em>31</em>(6), 751–751. <a href="https://doi.org/10.1017/s0140525x08006079">https://doi.org/10.1017/s0140525x08006079</a></p>
</div>
<div id="ref-mehrabian1974approach">
<p>Mehrabian, A., &amp; Russell, J. A. (1974). <em>An approach to environmental psychology.</em> MIT Press.</p>
</div>
<div id="ref-menninghaus2019">
<p>Menninghaus, W., Wagner, V., Wassiliwizky, E., Schindler, I., Hanich, J., Jacobsen, T., &amp; Koelsch, S. (2019). What are aesthetic emotions? <em>Psychological Review</em>, <em>126</em>(2), 171–195. <a href="https://doi.org/10.1037/rev0000135">https://doi.org/10.1037/rev0000135</a></p>
</div>
<div id="ref-russell1980">
<p>Russell, J. A. (1980). A circumplex model of affect. <em>Journal of Personality and Social Psychology</em>, <em>39</em>(6), 1161–1178. <a href="https://doi.org/10.1037/h0077714">https://doi.org/10.1037/h0077714</a></p>
</div>
<div id="ref-skov2020">
<p>Skov, M., &amp; Nadal, M. (2020). There are no aesthetic emotions: Comment on menninghaus et al. (2019). <em>Psychological Review</em>, <em>127</em>(4), 640–649. <a href="https://doi.org/10.1037/rev0000187">https://doi.org/10.1037/rev0000187</a></p>
</div>
<div id="ref-Zentner2008-mv">
<p>Zentner, M., Grandjean, D., &amp; Scherer, K. R. (2008). Emotions evoked by the sound of music: Characterization, classification, and measurement. <em>Emotion</em>, <em>8</em>(4), 494–521. <a href="https://doi.org/10.1037/1528-3542.8.4.494">https://doi.org/10.1037/1528-3542.8.4.494</a></p>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="music-across-the-world.html"><span class="header-section-number">14</span> Music across the world</a></div>
<div class="next"><a href="introduction-1.html"><span class="header-section-number">16</span> Introduction</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#emotion"><span class="header-section-number">15</span> Emotion</a></li>
<li>
<a class="nav-link" href="#introduction-to-emotion-and-music"><span class="header-section-number">15.1</span> Introduction to emotion and music</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#definitions"><span class="header-section-number">15.1.1</span> Definitions</a></li></ul>
</li>
<li>
<a class="nav-link" href="#modelling-emotion"><span class="header-section-number">15.2</span> Modelling emotion</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#dimensional-models"><span class="header-section-number">15.2.1</span> Dimensional models</a></li>
<li><a class="nav-link" href="#categorical-models"><span class="header-section-number">15.2.2</span> Categorical models</a></li>
<li><a class="nav-link" href="#dimensional-versus-categorical-models"><span class="header-section-number">15.2.3</span> Dimensional versus categorical models</a></li>
<li><a class="nav-link" href="#aesthetic-and-musical-emotions"><span class="header-section-number">15.2.4</span> Aesthetic and musical emotions</a></li>
<li><a class="nav-link" href="#conclusions-1"><span class="header-section-number">15.2.5</span> Conclusions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#measuring-emotional-responses"><span class="header-section-number">15.3</span> Measuring emotional responses</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#self-report"><span class="header-section-number">15.3.1</span> Self-report</a></li>
<li><a class="nav-link" href="#physiological"><span class="header-section-number">15.3.2</span> Physiological</a></li>
<li><a class="nav-link" href="#neuroimaging"><span class="header-section-number">15.3.3</span> Neuroimaging</a></li>
<li><a class="nav-link" href="#conclusions-3"><span class="header-section-number">15.3.4</span> Conclusions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#mechanisms-for-inducing-musical-emotion"><span class="header-section-number">15.4</span> Mechanisms for inducing musical emotion</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#brain-stem-reflex"><span class="header-section-number">15.4.1</span> Brain stem reflex</a></li>
<li><a class="nav-link" href="#rhythmic-entrainment"><span class="header-section-number">15.4.2</span> Rhythmic entrainment</a></li>
<li><a class="nav-link" href="#evaluative-conditioning"><span class="header-section-number">15.4.3</span> Evaluative conditioning</a></li>
<li><a class="nav-link" href="#emotional-contagion"><span class="header-section-number">15.4.4</span> Emotional contagion</a></li>
<li><a class="nav-link" href="#visual-imagery"><span class="header-section-number">15.4.5</span> Visual imagery</a></li>
<li><a class="nav-link" href="#episodic-memory"><span class="header-section-number">15.4.6</span> Episodic memory</a></li>
<li><a class="nav-link" href="#musical-expectancy"><span class="header-section-number">15.4.7</span> Musical expectancy</a></li>
<li><a class="nav-link" href="#aesthetic-judgment"><span class="header-section-number">15.4.8</span> Aesthetic judgment</a></li>
</ul>
</li>
<li><a class="nav-link" href="#conclusions-4"><span class="header-section-number">15.5</span> Conclusions</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/pmcharrison/intro-to-music-and-science/blob/main/060-emotion.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/pmcharrison/intro-to-music-and-science/edit/main/060-emotion.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Music and Science</strong>" was written by Peter M. C. Harrison. It was last built on 2024-01-03.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
